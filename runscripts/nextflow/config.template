// Global default params
params {
    experimentId = 'EXPERIMENT_ID'
    config = 'CONFIG_FILE'
}

trace {
    enabled = true
    fields = 'name,native_id,status,submit,start,complete,duration,realtime,exit,realtime,%cpu,%mem,rss,peak_rss,error_action,attempt,cpu_model,workdir'
}

profiles {
    gcloud {
        // Retry once with more RAM if OOM
        process.memory = { 4.GB * task.attempt }
        // Using single core is slightly slower but much cheaper
        process.cpus = 1
        process.executor = 'google-batch'
        process.container = 'IMAGE_NAME'
        process.errorStrategy = {
            // Codes: 137 (out-of-memory)
            ((task.exitStatus == 137)
            && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore' }
        google.project = 'allen-discovery-center-mcovert'
        google.location = 'us-west1'
        google.batch.spot = true
        docker.enabled = true
        params.projectRoot = '/vivarium-ecoli'
        params.publishDir = "PUBLISH_DIR"
        process.maxRetries = 1
        // Check Google Cloud latest spot pricing / performance
        process.machineType = 't2d-standard-1'
    }
    sherlock {
        process.memory = {
            if ( task.exitStatus in [137, 140] ) {
                4.GB * task.attempt
            } else {
                4.GB
            }
        }
        process.errorStrategy = {
            // Codes: 140 (SLURM job limits), 143 (SLURM preemption)
            // Default value for exitStatus is max integer value, this
            // is a catch-all for errors that leave no exit code
            ((task.exitStatus in [140, 143, Integer.MAX_VALUE])
            && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore' }
        // Using single core is slightly slower but can have shorter
        // queue times and is less damaging to future job priority
        process.cpus = 1
        process.executor = 'slurm'
        process.queue = 'owners'
        process.time = {
            if ( task.exitStatus == 140 ) {
                2.h * task.attempt
            } else {
                2.h
            }
        }
        process.maxRetries = 3
        params.projectRoot = "${launchDir}"
        params.publishDir = "PUBLISH_DIR"
        // Avoid getting queue status too frequently (can cause job status mixups)
        executor.queueStatInterval = '2 min'
        // Check for terminated jobs and submit new ones less frequently
        // than we check queue status (ensure correct and latest status) 
        executor.pollInterval = '5 min'
        // Retry all jobs that fail to submit (different from fail during runtime)
        executor.submit.retry.reason = '.*'
        // Retry failed submissions with delay longer than the time to
        // get latest correct queue status (avoid job status mixups)
        executor.retry.delay = '5 min'
        executor.retry.maxDelay = '10 min'
        // Throttle submission rate to avoid overwhelming scheduler
        executor.submitRateLimit = '20/min'
        // Give NFS time to update and sync before raising errors
        executor.exitReadTimeout = '15 min'
        // Write out job status to log file less frequently
        // than we check queue status (log correct and latest status)
        executor.dumpInterval = '6 min'
    }
    standard {
        process.executor = 'local'
        params.projectRoot = "${launchDir}"
        params.publishDir = "PUBLISH_DIR"
    }
}
