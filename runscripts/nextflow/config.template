// Global default params
params {
    experimentId = 'EXPERIMENT_ID'
    config = 'CONFIG_FILE'
    parca_cpus = PARCA_CPUS
    publishDir = 'PUBLISH_DIR'
    container_image = 'IMAGE_NAME'
    hyperqueue = false
}

trace.enabled = true
// Report dates/times in ms and memory in bytes for easier analysis
trace.raw = true
trace.sep = ','
trace.fields = 'name,native_id,status,submit,start,complete,duration,realtime,exit,%cpu,%mem,rss,peak_rss,error_action,attempt,cpu_model,workdir'
trace.file = "trace--${params.experimentId}--" + new java.text.SimpleDateFormat("yyyy-MM-dd--HH-mm-ss").format(new Date()) + ".csv"

profiles {
    gcloud {
        process {
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
            }
            errorStrategy = {
                // Codes: 137 (out-of-memory), 50001 - 50006 (Google Batch task fail:
                // https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes)
                (((task.exitStatus == 137) || (task.exitStatus >= 50001 && task.exitStatus <= 50006))
                && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore'
            }
            // Retry once with more RAM if OOM
            memory = { 4.GB * task.attempt }
            maxRetries = 1
            // Using single core is slightly slower but much cheaper
            cpus = 1
            executor = 'google-batch'
            container = params.container_image
            // Necessary otherwise symlinks to other files in bucket can break
            containerOptions = '--volume /mnt/disks/BUCKET:/mnt/disks/BUCKET'
            // Check Google Cloud latest spot pricing / performance
            machineType = {
                def cpus = task.cpus
                def powerOf2 = 1
                while (powerOf2 < cpus && powerOf2 < 64) {
                    powerOf2 *= 2
                }
                return "t2d-standard-${powerOf2}"
            }
        }
        // For this script to work on a Compute Engine VM, you must
        // - Set default Compute Engine region and zone for your project
        // - Set access scope to "Allow full access to all Cloud APIs" when
        //   creating VM
        // - Run gcloud init in VM
        google.project = {
            def project_proc = "gcloud config get project".execute()
            project_proc.waitFor()
            return project_proc.text.trim()
        }()
        google.location = {
            def location_proc = "gcloud config get compute/region".execute()
            location_proc.waitFor()
            return location_proc.text.trim()
        }()
        google.batch.spot = true
        google.batch.usePrivateAddress = true
        google.batch.network = 'global/networks/default'
        google.batch.subnetwork = "regions/${google.location}/subnetworks/default"
        docker.enabled = true
        params.projectRoot = '/vEcoli'
        workflow.failOnIgnore = true
    }
    sherlock {
        process {
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
                time = {
                    if ( task.exitStatus == 140 ) {
                        1.h * task.attempt
                    } else {
                        1.h
                    }
                }
            }
            container = params.container_image
            containerOptions = "-B ${params.publishDir}:${params.publishDir} -B ${launchDir}:${launchDir}"
            queue = 'owners,normal'
            // Run on newer, faster CPUs
            clusterOptions = '--prefer="CPU_GEN:GEN|CPU_GEN:SPR" --constraint="CPU_GEN:RME|CPU_GEN:MLN|CPU_GEN:BGM|CPU_GEN:SIE|CPU_GEN:GEN|CPU_GEN:SPR"'
            executor = 'slurm'
            cpus = 1
            memory = {
                if ( task.exitStatus in [137, 140] ) {
                    4.GB * task.attempt
                } else {
                    4.GB
                }
            }
            time = {
                if ( task.exitStatus == 140 ) {
                    4.h * task.attempt
                } else {
                    4.h
                }
            }
            errorStrategy = {
                // Codes: 137(OOM), 140 (SLURM job limits), 143 (SLURM preemption)
                // Default value for exitStatus is max integer value, this
                // is a catch-all for errors that leave no exit code
                ((task.exitStatus in [137, 140, 143, Integer.MAX_VALUE])
                && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore'
            }
            maxRetries = 3
        }
        apptainer.enabled = true
        params.projectRoot = '/vEcoli'
        // Avoid getting queue status too frequently (can cause job status mixups)
        executor.queueStatInterval = '2 min'
        // Check for terminated jobs and submit new ones fairly frequently
        // to minimize downtime between dependent jobs
        executor.pollInterval = '30 sec'
        // Retry all jobs that fail to submit (different from fail during runtime)
        executor.submit.retry.reason = '.*'
        // Retry failed submissions with delay longer than the time to
        // get latest queue status (avoid job status mixups)
        executor.retry.delay = '3 min'
        executor.retry.maxDelay = '5 min'
        // Throttle submission rate to avoid overwhelming scheduler
        executor.submitRateLimit = '20/min'
        // Raise limit to maximum allowed on Sherlock normal partition
        executor.queueSize = 2000
        // Give NFS time to update and sync before raising errors
        executor.exitReadTimeout = '10 min'
        workflow.failOnIgnore = true
    }
    sherlock_hq {
        process {
            // Run ParCa in separate SLURM job to allow >1 CPU
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
                executor = 'slurm'
                queue = 'owners,normal'
                // Run on newer, faster CPUs
                clusterOptions = '--prefer="CPU_GEN:GEN|CPU_GEN:SPR" --constraint="CPU_GEN:RME|CPU_GEN:MLN|CPU_GEN:BGM|CPU_GEN:SIE|CPU_GEN:GEN|CPU_GEN:SPR"'
                time = {
                    if ( task.exitStatus == 140 ) {
                        1.h * task.attempt
                    } else {
                        1.h
                    }
                }
            }
            // Creating variants happens before the HyperQueue workers
            // are allocated and requires a separate SLURM job
            // Analyses do not hold up the workflow and can be
            // scheduled on separate SLURM jobs to keep the HyperQueue
            // workers fully focused on running simulations
            withLabel: slurm_submit {
                executor = 'slurm'
                queue = 'owners,normal'
                clusterOptions = ''
            }
            container = params.container_image
            containerOptions = "-B ${params.publishDir}:${params.publishDir} -B ${launchDir}:${launchDir}"
            // Use Nextflow's retry logic instead of HyperQueue's built-in logic
            clusterOptions = '--crash-limit=1'
            cpus = 1
            memory = {
                if ( task.exitStatus in [137, 140] ) {
                    4.GB * task.attempt
                } else {
                    4.GB
                }
            }
            time = {
                if ( task.exitStatus == 140 ) {
                    4.h * task.attempt
                } else {
                    4.h
                }
            }
            errorStrategy = {
                ((task.exitStatus in [137, 140, 143, Integer.MAX_VALUE])
                && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore'
            }
            maxRetries = 3
        }
        params.hyperqueue = true
        // Raise limit to maximum allowed on Sherlock normal partition
        executor.queueSize = 2000
        apptainer.enabled = true
        params.projectRoot = '/vEcoli'
        workflow.failOnIgnore = true
    }
    standard {
        params.projectRoot = "${launchDir}"
        workflow.failOnIgnore = true
        process {
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
            }
            executor = 'local'
            errorStrategy = 'ignore'
        }
    }
}
