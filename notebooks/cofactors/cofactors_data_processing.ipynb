{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:44.705392Z",
     "start_time": "2024-05-17T18:06:44.616509Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pprint\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "import requests\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "os.chdir(os.path.expanduser('~/vivarium-ecoli'))\n",
    "\n",
    "ALLOWED_METAL_NAMES =   {'Iron': 'FE+2', 'Cobalt': 'CO+2', 'Copper': 'CU+2', 'Manganese': 'MN+2', 'Molybdenum': 'CPD-8123', 'Nickel': 'NI+2', 'Zinc': 'ZN+2',\n",
    "                        'Calcium': 'CA+2', 'Magnesium': 'MG+2', 'Sodium': 'NA+', 'Potassium': 'K+',\n",
    "                        'Iron-sulfur \\(4Fe-4S\\)': 'CPD-7', 'Iron-sulfur \\(2Fe-2S\\)': 'CPD-6',\n",
    "                         'Iron-sulfur \\(4Fe-4S-S-AdoMet\\)': 'CPD-7', 'Iron-sulfur \\(3Fe-4S\\)': '3FE-4S', 'Iron-oxo-sulfur \\(4Fe-2O-2S\\)': 'CPD-7',\n",
    "                         'Iron-sulfur': 'CPD-7', # has to be after others since it is a substring of others\n",
    "                        'heme': 'Heme-b', 'Molybdate': 'CPD-3', 'heme B': 'Heme-b', 'Cobalamin': 'COB-I-ALAMIN',\n",
    "                         'Selenocysteine': 'L-SELENOCYSTEINE',\n",
    "                        'Divalent metal cation': 'Any+2'}\n",
    "\n",
    "\n",
    "ACCEPTED_OTHER_FEATURES = {'PYRIDOXAL_PHOSPHATE', 'THIAMINE-PYROPHOSPHATE', 'FMN', 'FAD', 'LIPOIC-ACID', 'BIOTIN'}\n",
    "\n",
    "AMINO_ACID_MAP = {'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU', 'F': 'PHE', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
    "                  'K': 'LYS', 'L': 'LEU', 'M': 'MET', 'N': 'ASN', 'P': 'PRO', 'Q': 'GLN', 'R': 'ARG', 'S': 'SER',\n",
    "                  'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR', 'U': 'SEL', '*': 'TER'}\n",
    "\n",
    "# Create a list of temporarily allowed Gene Ontology terms to fix gaps in pathway annotations. Usually non-metabolic\n",
    "# Currently: Tx Reg, Transcription, translation, DNA replication, Cell division, iron-sulfur cluster assembly, proteolysis, dna repair, copper response\n",
    "TEMP_GO_TERMS = {'GO:0006355': 'Regulation of transcription', 'GO:0010468': 'Regulation of transcription',\n",
    "                  'GO:0006351': 'Transcription', 'GO:0006350': 'Transcription',\n",
    "                  'GO:0006412': 'Translation', 'GO:0006260': 'DNA replication', 'GO:0045454': 'Redox homeostasis',\n",
    "                  'GO:0051301': 'Cell division', 'GO:0015288': 'Porin',\n",
    "                  'GO:0009451': 'RNA modification', 'GO:0006400': 'tRNA modification', 'GO:0008033': 'tRNA processing',\n",
    "                  'GO:0018339': 'Ribosome biogenesis', 'GO:0042254': 'Ribosome biogenesis', 'GO:0006364': 'rRNA processing',\n",
    "                  'GO:0016226': 'Iron-sulfur cluster assembly', 'GO:0006508': 'Proteolysis', 'GO:0006281': 'DNA repair',\n",
    "                  'GO:0006879': 'Iron homeostasis', 'GO:0033214': 'Iron homeostasis', 'GO:0015685': 'Iron homeostasis',  'GO:0015687': 'Iron homeostasis',\n",
    "                  # 'GO:0006457': 'Protein folding', \n",
    "                  # 'GO:0030091': 'Protein repair',\n",
    "                  # 'GO:0006605': 'Protein targeting', ' GO:0015031': 'Protein transport', \n",
    "                  # 'GO:0051205': 'Protein insertion', \n",
    "                 # summarize those in one term\n",
    "                  'GO:0006457': 'Protein folding, localization and repair', 'GO:0030091': 'Protein folding, localization and repair', \n",
    "                  'GO:0006605': 'Protein folding, localization and repair', 'GO:0015031': 'Protein folding, localization and repair', \n",
    "                  'GO:0051205': 'Protein folding, localization and repair',\n",
    "                  'GO:0046688': 'Response to copper ion', \n",
    "                  'GO:ZZZ': 'Non-porin small-molecule transport', 'GO:0071702': 'Non-porin small-molecule transport',\n",
    "                  'GO:STRUCTURE': 'Structural maintenance',}\n",
    "\n",
    "# \n",
    "\n",
    "# as residues\n",
    "AMINO_ACID_RESIDUE_MASSES = {\n",
    "    'ALA': 71.03711, 'ARG': 156.10111, 'ASN': 114.04293, 'ASP': 115.02694,\n",
    "    'CYS': 103.00919, 'GLU': 129.04259, 'GLN': 128.05858, 'GLY': 57.02146,\n",
    "    'HIS': 137.05891, 'ILE': 113.08406, 'LEU': 113.08406, 'LYS': 128.09496,\n",
    "    'MET': 131.04049, 'PHE': 147.06841, 'PRO': 97.05276, 'SER': 87.03203,\n",
    "    'THR': 101.04768, 'TRP': 186.07931, 'TYR': 163.06333, 'VAL': 99.06841,\n",
    "    'SEL': 150.0379, 'TER': 0.0\n",
    "}\n",
    "\n",
    "\n",
    "erroneous_monomer_metal_interactions = [\"EG11415-MONOMER\", \"EG12132-MONOMER\",\n",
    "                                        \"EG12332-MONOMER\", \"EG11378-MONOMER\", \"G7748-MONOMER\", \n",
    "                                        \"CRR-MONOMER\", \"EG11663-MONOMER\", \"GLYOXI-MONOMER\",\n",
    "                                        \"EG10697-MONOMER\", \"EG12310-MONOMER\", \"EG10694-MONOMER\", \"EG10698-MONOMER\", #Mn\n",
    "                                        \"PPENTOMUT-MONOMER\", ] #Mn \n",
    "\n",
    "erroneous_pathway_annotations = ['EG10695-MONOMER']\n",
    "\n",
    "\n",
    "\n",
    "def get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, name_list, level_vector, level=2, parent_dict=None):\n",
    "\n",
    "    if parent_dict is None:\n",
    "        parent_dict = {}\n",
    "\n",
    "    cur_pathway_level = level_vector[cur_pathway_idx]\n",
    "\n",
    "    if cur_pathway_level == level:\n",
    "        parent_dict[name_list[cur_pathway_idx]] = cur_pathway_level\n",
    "\n",
    "    parent_slice = pathway_matrix[:, cur_pathway_idx]\n",
    "    parent_idxs = np.where(parent_slice != 0)[0]\n",
    "\n",
    "\n",
    "    for idx in parent_idxs:\n",
    "\n",
    "        _ = get_pathway_ith_level_parents(idx, pathway_matrix, name_list, level_vector, level, parent_dict)\n",
    "\n",
    "    return parent_dict"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_complex_df = pd.read_csv('notebooks/cofactors/data/raw_complexes.csv', index_col=False)\n",
    "\n",
    "# read stoichiometry, cofactors and enzyme_reaction as literal sets\n",
    "for column in ['stoichiometry', 'cofactors', 'enzyme_reaction']:\n",
    "    parsed_complex_df[column] = parsed_complex_df[column].apply(ast.literal_eval)\n",
    "\n",
    "parsed_protein_df = pd.read_csv('notebooks/cofactors/data/raw_proteins.csv', index_col=False)\n",
    "\n",
    "for column in ['cofactors', 'enzyme_reaction', 'metal_features', 'other_features', 'direct_annotations', 'go_annotations']:\n",
    "    parsed_protein_df[column] = parsed_protein_df[column].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "parsed_cofactor_df = pd.read_csv('notebooks/cofactors/data/raw_cofactors.csv', index_col=False)\n",
    "\n",
    "for column in ['elemental_composition']:\n",
    "    parsed_cofactor_df[column] = parsed_cofactor_df[column].apply(ast.literal_eval)\n",
    "\n",
    "parsed_pathway_df = pd.read_csv('notebooks/cofactors/data/raw_pathways.csv', index_col=False)\n",
    "\n",
    "for column in ['parents', 'children']:\n",
    "    parsed_pathway_df[column] = parsed_pathway_df[column].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add GO \"pathways\" to pathway df\n",
    "go_collection = []\n",
    "already_seen = []\n",
    "go_collection.append({'id': 'Other functions', 'common_name': 'Other functions', 'level': 1, 'parents': [], 'children': []})\n",
    "\n",
    "for go_term, go_name in TEMP_GO_TERMS.items():\n",
    "    if go_name not in already_seen:\n",
    "        already_seen.append(go_name)\n",
    "        go_collection[0]['children'].append(go_name)\n",
    "        \n",
    "        go_name_child = go_name + ' (child)'\n",
    "        \n",
    "        go_collection.append({'id': go_name, 'common_name': go_name, 'level': 2, 'parents': ['Other functions'], 'children': [go_name_child]})\n",
    "        go_collection.append({'id': go_name_child, 'common_name': go_name_child, 'level': 3, 'parents': [go_name], 'children': []})\n",
    "\n",
    "go_df = pd.DataFrame(go_collection)\n",
    "\n",
    "parsed_pathway_df = pd.concat([parsed_pathway_df, go_df], ignore_index=True)\n",
    "# parsed_pathway_df\n",
    "\n",
    "\n",
    "# add membrane protein areas\n",
    "membrane_monomers = pd.read_csv('notebooks/cofactors/data/monomer_vs_area.csv', index_col=False)\n",
    "membrane_complexes = pd.read_csv('notebooks/cofactors/data/complex_vs_area.csv', index_col=False)\n",
    "\n",
    "# rename complex column in membrane_complexes to id\n",
    "membrane_complexes = membrane_complexes.rename(columns={'complex': 'id'})\n",
    "\n",
    "# concat, pick out id and area_trans\n",
    "membrane_proteins = pd.concat([membrane_monomers, membrane_complexes], ignore_index=True)\n",
    "membrane_proteins = membrane_proteins.loc[:, ['id', 'area_trans']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:44.914412Z",
     "start_time": "2024-05-17T18:06:44.707140Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:44.916729Z",
     "start_time": "2024-05-17T18:06:44.915296Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing into final tables\n",
    "## Specific adjustments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# removing specific troublesome interactions that have been discovered in the data\n",
    "# should not be in model\n",
    "\n",
    "\n",
    "# remove metal features from EG11415-MONOMER in parsed_protein_df\\\n",
    "for protein in erroneous_monomer_metal_interactions:\n",
    "    prot_idx = np.where(parsed_protein_df['id'] == protein)[0][0]\n",
    "    parsed_protein_df.at[prot_idx, 'metal_features'] = []\n",
    "\n",
    "# classify folE gene use as cofactor production (THF)\n",
    "pathway_idx = parsed_pathway_df[parsed_pathway_df['id'] == '6-HM-Dihydropterin-PP-Biosynthesis'].index[0]\n",
    "parsed_pathway_df.at[pathway_idx, 'parents'] = ['Cofactor-Biosynthesis']\n",
    "\n",
    "protein_idx = parsed_protein_df[parsed_protein_df['id'] == 'GTP-CYCLOHYDRO-I-MONOMER'].index[0]\n",
    "parsed_protein_df.at[protein_idx, 'direct_annotations'] = set(['PWY-6147'])\n",
    "\n",
    "# remove pathway from erroneous pathway annotations\n",
    "for protein in erroneous_pathway_annotations:\n",
    "    prot_idx = np.where(parsed_protein_df['id'] == protein)[0][0]\n",
    "    parsed_protein_df.at[prot_idx, 'direct_annotations'] \n",
    "\n",
    "# for all proteins with \"port\" in common_name, add \"GO:ZZZ\" to go_annotations\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    common_name = parsed_protein_df.at[i, 'common_name']\n",
    "    if type(common_name) is str and 'port' in common_name.lower():\n",
    "        parsed_protein_df.at[i, 'go_annotations'].add('GO:ZZZ')\n",
    "\n",
    "# add lpp to structural maintenance\n",
    "for protein in ['EG10544-MONOMER', 'EG12117-MONOMER']:\n",
    "    prot_idx = np.where(parsed_protein_df['id'] == protein)[0][0]\n",
    "    parsed_protein_df.at[prot_idx, 'go_annotations'] = set(['GO:STRUCTURE'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:44.936737Z",
     "start_time": "2024-05-17T18:06:44.917426Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add membrane protein areas to parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process raw EcoCyc annotations into standard EcoCyc names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# remove all \\ characters from keys in ALLOWED_METAL_NAMES\n",
    "NON_REGEX_METAL = {key.replace('\\\\', ''): value for key, value in ALLOWED_METAL_NAMES.items()}\n",
    "\n",
    "parsed_protein_df['metal_features_processed'] = 0\n",
    "parsed_protein_df['metal_features_processed'] = parsed_protein_df['metal_features_processed'].astype(object)\n",
    "\n",
    "metal_pattern = '|'.join(ALLOWED_METAL_NAMES.keys())\n",
    "metal_regex = re.compile(f'(({metal_pattern})(\\s\\d[\\.,;]|[\\.,;]|\\s\\())')\n",
    "\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    metal_binding = parsed_protein_df.loc[i, 'metal_features']\n",
    "\n",
    "    metal_count_dict = {}\n",
    "    existing_matches = set()\n",
    "\n",
    "    for feature in metal_binding:\n",
    "        matches = metal_regex.search(feature)\n",
    "        if matches:\n",
    "            metal = matches.group(0)[:-1]\n",
    "\n",
    "            # eliminate duplicates\n",
    "            if metal not in existing_matches:\n",
    "\n",
    "                existing_matches.add(metal)\n",
    "\n",
    "                if 'heme' in feature:\n",
    "                    metal = metal.replace('Iron', 'heme')\n",
    "                if 'alamin' in feature:\n",
    "                    metal = metal.replace('Cobalt', 'Cobalamin')\n",
    "\n",
    "                # check if last char of metal is a number, then crop\n",
    "                if metal[-1].isdigit():\n",
    "                    metal = metal[:-2]\n",
    "\n",
    "                metal = metal.strip()\n",
    "\n",
    "                # replace metal name with allowed metal name\n",
    "                metal = NON_REGEX_METAL[metal]\n",
    "\n",
    "                if metal in metal_count_dict:\n",
    "                    metal_count_dict[metal] += 1\n",
    "                else:\n",
    "                    metal_count_dict[metal] = 1\n",
    "\n",
    "        else:\n",
    "            print(f'No match for {feature} in {parsed_protein_df.loc[i, \"id\"]}')\n",
    "\n",
    "\n",
    "\n",
    "    # EXCEPTIONS\n",
    "    # if both magnesium and manganese are present, replace with magnesium\n",
    "    # TODO remove when using UniProt data. Ecocyc data is not as reliable\n",
    "    if 'MG+2' in metal_count_dict and 'MN+2' in metal_count_dict and metal_count_dict['MG+2'] == metal_count_dict['MN+2']:\n",
    "        del metal_count_dict['MN+2']\n",
    "    # same with cobalt\n",
    "    if 'CO+2' in metal_count_dict and 'MG+2' in metal_count_dict and metal_count_dict['CO+2'] == metal_count_dict['MG+2']:\n",
    "        del metal_count_dict['CO+2']\n",
    "    elif 'CO+2' in metal_count_dict:\n",
    "        metal_count_dict['MG+2'] = metal_count_dict['CO+2']\n",
    "        del metal_count_dict['CO+2']\n",
    "\n",
    "    parsed_protein_df.at[i, 'metal_features_processed'] = metal_count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.035426Z",
     "start_time": "2024-05-17T18:06:44.937568Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# manual corrections in data\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == '3-OXOACYL-ACP-REDUCT-MONOMER'][0]\n",
    "\n",
    "# remove cofactors\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {}\n",
    "\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == 'CARBPSYN-LARGE'][0]\n",
    "\n",
    "# remove cofactors\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed']\n",
    "mn_cofactor_count = parsed_protein_df.at[correction_idx, 'metal_features_processed']['MN+2']\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {'MG+2': mn_cofactor_count}\n",
    "\n",
    "# dps\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == 'EG11415-MONOMER'][0]\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.039932Z",
     "start_time": "2024-05-17T18:06:45.036304Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_protein_df = parsed_protein_df.drop(columns=['metal_features'])\n",
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.059091Z",
     "start_time": "2024-05-17T18:06:45.040911Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_protein_df['other_features_processed'] = 0\n",
    "parsed_protein_df['other_features_processed'] = parsed_protein_df['other_features_processed'].astype(object)\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    other_features = parsed_protein_df.loc[i, 'other_features']\n",
    "\n",
    "    other_feature_count_dict = {}\n",
    "    existing_matches = set()\n",
    "\n",
    "    for feature in other_features:\n",
    "\n",
    "        # eliminate duplicates\n",
    "        if feature not in existing_matches:\n",
    "\n",
    "            existing_matches.add(feature)\n",
    "\n",
    "            if feature in ACCEPTED_OTHER_FEATURES:\n",
    "                if feature in other_feature_count_dict:\n",
    "                    other_feature_count_dict[feature] += 1\n",
    "                else:\n",
    "                    other_feature_count_dict[feature] = 1\n",
    "\n",
    "    parsed_protein_df.at[i, 'other_features_processed'] = other_feature_count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.154389Z",
     "start_time": "2024-05-17T18:06:45.059995Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# processed go_annotations. if no pathways exist for monomer, use go annotations\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_go_annotations = parsed_protein_df.loc[i, 'go_annotations']\n",
    "    cur_pathways = parsed_protein_df.loc[i, 'direct_annotations']\n",
    "\n",
    "    replacement_pathways = list()\n",
    "\n",
    "    if len(cur_pathways) == 0:\n",
    "\n",
    "        for go_term in TEMP_GO_TERMS:\n",
    "            for go_annotation in cur_go_annotations:\n",
    "                if go_term == go_annotation:\n",
    "                    replacement_pathways.append(TEMP_GO_TERMS[go_term] + ' (child)')\n",
    "\n",
    "        if len(replacement_pathways) > 0:\n",
    "            print(parsed_protein_df.at[i, \"id\"], set([replacement_pathways[0]]))\n",
    "            parsed_protein_df.at[i, 'direct_annotations'] = set([replacement_pathways[0]])\n",
    "\n",
    "\n",
    "# parsed_protein_df[parsed_protein_df['id'] == 'PD00197']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.293490Z",
     "start_time": "2024-05-17T18:06:45.158034Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_protein_df.loc[parsed_protein_df['id'] == 'EG10230-MONOMER']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.300019Z",
     "start_time": "2024-05-17T18:06:45.294262Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Add uncategorized pathways\n",
    "new_rows = [\n",
    "    {'id': 'Uncategorized (I)', 'parents': [], 'children': ['Uncategorized (II)'], 'level': 1, 'common_name': 'Uncategorized (I)'},\n",
    "    {'id': 'Uncategorized (II)', 'parents': ['Uncategorized (I)'], 'children': ['Uncategorized (III)'], 'level': 2, 'common_name': 'Uncategorized'},\n",
    "    {'id': 'Uncategorized (III)', 'parents': ['Uncategorized (II)'], 'children': [], 'level': 3, 'common_name': 'Uncategorized (III)'},\n",
    "]\n",
    "\n",
    "# Convert the dictionaries to a DataFrame.\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Insert the new rows into the existing DataFrame.\n",
    "# Assuming you want to append at the end:\n",
    "parsed_pathway_df = pd.concat([parsed_pathway_df, new_rows_df], ignore_index=True)\n",
    "\n",
    "# for all proteins with empty direct_annotations, add 'Uncategorized (III)'\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    if len(parsed_protein_df.at[i, 'direct_annotations']) == 0:\n",
    "        parsed_protein_df.at[i, 'direct_annotations'] = set(['Uncategorized (III)'])\n",
    "        \n",
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.356934Z",
     "start_time": "2024-05-17T18:06:45.300768Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_protein_df.loc[parsed_protein_df['id'] == 'EG10230-MONOMER']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.363823Z",
     "start_time": "2024-05-17T18:06:45.357913Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# decompose sequence into dict of amino acid counts\n",
    "parsed_protein_df['sequence_processed'] = 0\n",
    "parsed_protein_df['sequence_processed'] = parsed_protein_df['sequence_processed'].astype(object)\n",
    "\n",
    "parsed_protein_df['sequence_mass'] = 0\n",
    "\n",
    "unique_aa = set()\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    sequence = parsed_protein_df.loc[i, 'seq']\n",
    "\n",
    "    if type(sequence) != str:\n",
    "        print(f'No sequence for {parsed_protein_df.loc[i, \"id\"]}')\n",
    "        continue\n",
    "\n",
    "    aa_count_dict = {}\n",
    "\n",
    "    mass = 0\n",
    "\n",
    "    for aa in sequence:\n",
    "        if aa in aa_count_dict:\n",
    "            aa_count_dict[aa] += 1\n",
    "        else:\n",
    "            aa_count_dict[aa] = 1\n",
    "\n",
    "        if aa not in unique_aa:\n",
    "            unique_aa.add(aa)\n",
    "\n",
    "        mass += AMINO_ACID_RESIDUE_MASSES[AMINO_ACID_MAP[aa]]\n",
    "\n",
    "    parsed_protein_df.at[i, 'sequence_processed'] = aa_count_dict\n",
    "    parsed_protein_df.at[i, 'sequence_mass'] = mass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.789515Z",
     "start_time": "2024-05-17T18:06:45.364810Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_protein_df = parsed_protein_df.drop(columns=['other_features','seq'])\n",
    "\n",
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.805895Z",
     "start_time": "2024-05-17T18:06:45.790295Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create new column for monomer component stoichiometry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "complex_ids = parsed_complex_df['id'].tolist()\n",
    "monomer_names = parsed_protein_df['id'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.808827Z",
     "start_time": "2024-05-17T18:06:45.806747Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def recursive_component_tree(current_component_name, complex_table, protein_table,\n",
    "                             current_multiplier=1, component_list=None, parent=None, return_cofactors=False):\n",
    "    \"\"\"\n",
    "    Recursively find all downstream components of a given complex.\n",
    "    \"\"\"\n",
    "\n",
    "    complex_names = complex_table['id'].tolist()\n",
    "    monomer_names = protein_table['id'].tolist()\n",
    "\n",
    "\n",
    "    my_children = {}\n",
    "\n",
    "    if component_list is None:\n",
    "        component_list = []\n",
    "\n",
    "\n",
    "    if current_component_name in complex_names:\n",
    "\n",
    "\n",
    "        cplx_idx = complex_table.index[complex_table['id'] == current_component_name][0]\n",
    "        stoichiometry = complex_table.at[cplx_idx, 'stoichiometry']\n",
    "\n",
    "        direct_children = {k: abs(v) for k, v in stoichiometry.items() if v < 0}\n",
    "\n",
    "        for component_name, coefficient in stoichiometry.items():\n",
    "\n",
    "            if coefficient < 0 and component_name != current_component_name:\n",
    "\n",
    "                child_multiplier = abs(coefficient * current_multiplier)\n",
    "\n",
    "                new_child = recursive_component_tree(component_name, complex_table, protein_table,\n",
    "                                                     child_multiplier, component_list, current_component_name, return_cofactors)\n",
    "\n",
    "                my_children = my_children | new_child\n",
    "\n",
    "\n",
    "            elif coefficient > 0 and component_name == current_component_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"key {component_name} and value {coefficient} for complex {component_name} not processed properly.\")\n",
    "\n",
    "        component_list.append({'name': current_component_name,'parent': parent, 'children': direct_children,\n",
    "                               'multiplier': int(current_multiplier), })\n",
    "\n",
    "\n",
    "    elif current_component_name in monomer_names:\n",
    "\n",
    "        # TODO check if enzrxn\n",
    "        if return_cofactors:\n",
    "            protein_idx = protein_table.index[protein_table['id'] == current_component_name][0]\n",
    "\n",
    "            protein_metals = protein_table.at[protein_idx, 'metal_features_processed']\n",
    "            protein_other = protein_table.at[protein_idx, 'other_features_processed']\n",
    "\n",
    "            table_cofactors = protein_metals | protein_other\n",
    "\n",
    "            if len(table_cofactors) > 0:\n",
    "                # TODO Add apo protein to component list\n",
    "                my_children = {}\n",
    "\n",
    "                for cofactor, cofactor_coefficient in table_cofactors.items():\n",
    "                    if table_cofactors[cofactor] !=  None:\n",
    "                        my_children[cofactor] = cofactor_coefficient\n",
    "                        component_list.append({'parent': current_component_name,\n",
    "                                               'name': cofactor,\n",
    "                                               'multiplier': abs(int(current_multiplier * cofactor_coefficient)),\n",
    "                                               'children': None})\n",
    "\n",
    "            component_list.append({'parent': parent, 'name': current_component_name, 'multiplier': current_multiplier, 'children': my_children})\n",
    "\n",
    "        else:\n",
    "            my_children = None\n",
    "            component_list.append({'parent': parent, 'name': current_component_name, 'multiplier': current_multiplier, 'children': None})\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"component {current_component_name} not found in complex or protein tables\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "    if parent is None:\n",
    "        return {current_component_name: my_children}, component_list\n",
    "    else:\n",
    "        return {current_component_name: my_children}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.814672Z",
     "start_time": "2024-05-17T18:06:45.809428Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "complex_tree_structure, nodes = recursive_component_tree('CPLX0-8167', parsed_complex_df, parsed_protein_df)\n",
    "pp.pprint(nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:45.818260Z",
     "start_time": "2024-05-17T18:06:45.815445Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_complex_df['monomer_component_stoichiometry'] = 0\n",
    "parsed_complex_df['monomer_component_stoichiometry'] = parsed_complex_df['monomer_component_stoichiometry'].astype(object)\n",
    "\n",
    "for i in range(len(parsed_complex_df.index)):\n",
    "    complex_name = parsed_complex_df.loc[i, 'id']\n",
    "    complex_tree_structure, nodes = recursive_component_tree(complex_name, parsed_complex_df, parsed_protein_df)\n",
    "\n",
    "    monomer_components = {node['name']: node['multiplier'] for node in nodes if node['children'] is None}\n",
    "\n",
    "    parsed_complex_df.at[i, 'monomer_component_stoichiometry'] = monomer_components"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:46.085022Z",
     "start_time": "2024-05-17T18:06:45.818925Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_complex_df = parsed_complex_df.loc[:, [\"id\", \"common_name\", \"stoichiometry\", \"monomer_component_stoichiometry\", \"cofactors\"]]\n",
    "parsed_complex_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:46.094261Z",
     "start_time": "2024-05-17T18:06:46.085755Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create tree matrix (also for Julia)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# save names\n",
    "complex_ids = list(parsed_complex_df['id'])\n",
    "protein_ids = list(parsed_protein_df['id'])\n",
    "cofactor_ids = list(parsed_cofactor_df['id'])\n",
    "pathway_ids = list(parsed_pathway_df['id'])\n",
    "\n",
    "\n",
    "name_idx = complex_ids + protein_ids + cofactor_ids\n",
    "tree_matrix = np.zeros([len(complex_ids) + len(protein_ids) + len(cofactor_ids), len(complex_ids) + len(protein_ids) + len(cofactor_ids)], dtype=np.int64)\n",
    "\n",
    "for i in range(len(parsed_complex_df)):\n",
    "    name = parsed_complex_df.at[i, 'id']\n",
    "    tree_structure, nodes = recursive_component_tree(name, parsed_complex_df, parsed_protein_df, return_cofactors=True)\n",
    "\n",
    "    for node in nodes:\n",
    "        node_name = node['name']\n",
    "        node_children = node['children']\n",
    "\n",
    "        if node_children != None:\n",
    "            for child_name, child_coefficient in node_children.items():\n",
    "                if child_name in name_idx:\n",
    "                        tree_matrix[name_idx.index(node_name), name_idx.index(child_name)] = child_coefficient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:46.883634Z",
     "start_time": "2024-05-17T18:06:46.095231Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create matrices to get cofactor counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "parsed_pathway_df[parsed_pathway_df['id'] == 'Regulation of transcription']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:46.888971Z",
     "start_time": "2024-05-17T18:06:46.884402Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "priority_list = ['Activation-Inactivation-Interconversion','Metabolic-Clusters', 'Macromolecule-Modification', 'Glycan-Pathways','Detoxification',  'Degradation']\n",
    "\n",
    "# pathway matrix is necessary to traverse tree\n",
    "pathway_matrix = np.zeros((len(pathway_ids), len(pathway_ids)), dtype=np.int64)\n",
    "level_vector = np.zeros(len(pathway_ids), dtype=np.int64)\n",
    "\n",
    "for i in range(len(parsed_pathway_df)):\n",
    "\n",
    "    cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "    level_vector[i] = parsed_pathway_df.at[i, 'level']\n",
    "\n",
    "    pathway_parents = parsed_pathway_df.at[i, 'parents']\n",
    "    pathway_children = parsed_pathway_df.at[i, 'children']\n",
    "\n",
    "    for parent in pathway_parents:\n",
    "        j = pathway_ids.index(parent)\n",
    "        pathway_matrix[j, i] = 1\n",
    "\n",
    "    for child in pathway_children:\n",
    "        j = pathway_ids.index(child)\n",
    "        pathway_matrix[i, j] = 1\n",
    "\n",
    "original_pathway_matrix = pathway_matrix.copy()\n",
    "\n",
    "# get superpathway indices\n",
    "super_pathway_idx = pathway_ids.index('Super-Pathways')\n",
    "super_pathway_children_idxs = np.where(pathway_matrix[super_pathway_idx, :] == 1)[0]\n",
    "\n",
    "# zero out all superpathway children\n",
    "pathway_matrix[:, super_pathway_children_idxs] = 0\n",
    "\n",
    "# for columns (children) with multiple parents, if one parent leads to degradation or glycans, remove it.\n",
    "for i in range(len(pathway_matrix[0, :])):\n",
    "    cur_pathway = pathway_ids[i]\n",
    "\n",
    "    if pathway_matrix[:, i].sum() > 1:\n",
    "\n",
    "        nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "        top_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_ids, level_vector, level=1).keys())[0] for j in nz_idxs]\n",
    "        # print(f\"multiple parents {top_level_classes} for {cur_pathway}\")\n",
    "\n",
    "\n",
    "        # when there are multiple parents, remove them in the following order of priority:\n",
    "        for priority in priority_list:\n",
    "            while priority in top_level_classes and len(nz_idxs) > 1:\n",
    "                priority_index = top_level_classes.index(priority)\n",
    "                pathway_matrix[nz_idxs[priority_index], i] = 0\n",
    "                nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "                top_level_classes[priority_index] = 'N/A'\n",
    "\n",
    "\n",
    "        # then, if there are still multiple parents, remove all but the first one\n",
    "        # TODO Change to parent with most frequently occuring 2nd parent.\n",
    "        if len(nz_idxs) > 1:\n",
    "            # top_two_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_name_list, level_vector, level=2).keys())[0] for j in nz_idxs]\n",
    "            # print(f\"multiple parents with 2nd level categories {top_two_level_classes} for {cur_pathway}\")\n",
    "            pathway_matrix[nz_idxs[1:], i] = 0\n",
    "\n",
    "        nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "        top_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_ids, level_vector, level=1).keys()) for j in nz_idxs]\n",
    "\n",
    "        # print(f\"pruned to {top_level_classes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:46.929939Z",
     "start_time": "2024-05-17T18:06:46.889745Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# if choice can be made, don't pick these. \n",
    "priority_list_second = ['SECONDARY-METABOLITE-BIOSYNTHESIS', 'Respiration', \n",
    "                        'Tetrapyrrole-Biosynthesis', 'Alcohol-Degradation', 'Carbohydrates-Biosynthesis']\n",
    "\n",
    "# create protein name to pathway mapping\n",
    "W = np.zeros((len(parsed_protein_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_pathways = parsed_protein_df.at[i, 'direct_annotations']\n",
    "\n",
    "    for pathway in cur_pathways:\n",
    "        pathway_idx = pathway_ids.index(pathway)\n",
    "        W[i, pathway_idx] = 1\n",
    "\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_protein_pathways_idxs = np.where(W[i, :] == 1)[0]\n",
    "    cur_protein = parsed_protein_df.at[i, 'id']\n",
    "\n",
    "    if len(cur_protein_pathways_idxs) < 2:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "\n",
    "        # for pathway_idx in cur_protein_pathways_idxs:\n",
    "\n",
    "        # get top level class of every pathway, and remove all but the first one of each class.\n",
    "\n",
    "        cur_protein_pathway_parents = list()\n",
    "        cur_protein_pathway_two_parents = list()\n",
    "\n",
    "        for pathway_idx in cur_protein_pathways_idxs:\n",
    "            top_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "            top_two_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "            print(f\"{cur_protein} has 2nd parents {top_two_parents} for {pathway_ids[pathway_idx]}\")\n",
    "\n",
    "            if len(top_parents) > 1:\n",
    "                print(f\"multiple parents {top_parents} for {pathway_ids[pathway_idx]} for {cur_protein}, should not happen.\")\n",
    "\n",
    "            if len(top_parents) == 1:\n",
    "                cur_protein_pathway_parents.append(list(top_parents.keys())[0])\n",
    "                cur_protein_pathway_two_parents.append(list(top_two_parents.keys())[0])\n",
    "            else:\n",
    "                # remove pathway with no parents\n",
    "                W[i, pathway_idx] = 0\n",
    "                cur_protein_pathway_parents.append('N/A')\n",
    "                cur_protein_pathway_two_parents.append('N/A')\n",
    "\n",
    "\n",
    "        # TODO - remove direct annotations with deprioritized parents\n",
    "        for priority in priority_list:\n",
    "            while priority in cur_protein_pathway_parents and len(np.where(W[i, :] == 1)[0]) > 1:\n",
    "                priority_index = cur_protein_pathway_parents.index(priority)\n",
    "                W[i, cur_protein_pathways_idxs[priority_index]] = 0\n",
    "                cur_protein_pathway_parents[priority_index] = 'N/A'\n",
    "\n",
    "        # same for 2nd level\n",
    "        for priority in priority_list_second:\n",
    "            while priority in cur_protein_pathway_two_parents and len(np.where(W[i, :] == 1)[0]) > 1:\n",
    "                priority_index = cur_protein_pathway_two_parents.index(priority)\n",
    "                W[i, cur_protein_pathways_idxs[priority_index]] = 0\n",
    "                cur_protein_pathway_two_parents[priority_index] = 'N/A'\n",
    "\n",
    "        new_protein_pathways_idxs = np.where(W[i, :] == 1)[0]\n",
    "\n",
    "        # for pathway_idx in cur_protein_pathways_idxs:\n",
    "\n",
    "        # get top level class of every pathway, and remove all but the first one of each class.\n",
    "\n",
    "        new_protein_pathway_parents = list()\n",
    "        new_protein_pathway_two_parents = list()\n",
    "\n",
    "        for pathway_idx in new_protein_pathways_idxs:\n",
    "            top_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "            top_two_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "            if len(top_parents) == 1:\n",
    "                new_protein_pathway_parents.append(list(top_parents.keys())[0])\n",
    "                new_protein_pathway_two_parents.append(list(top_two_parents.keys())[0])\n",
    "\n",
    "        print(f\"pruned to {cur_protein_pathway_two_parents}\")\n",
    "\n",
    "\n",
    "        # remove all N/A\n",
    "        # cur_protein_pathways_idxs = cur_protein_pathways_idxs[cur_protein_pathway_parents != 'N/A']\n",
    "\n",
    "        if len(np.unique(cur_protein_pathway_parents)) < 2:\n",
    "            continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.074857Z",
     "start_time": "2024-05-17T18:06:46.931155Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create pathway to 2nd layer pathway mapping\n",
    "W2 = np.zeros((len(parsed_pathway_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_pathway_df.index)):\n",
    "\n",
    "    cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "    cur_pathway_idx = pathway_ids.index(cur_pathway)\n",
    "\n",
    "    # get 2nd level parents\n",
    "    cur_pathway_parents = get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "    # if len(cur_pathway_parents) > 1:\n",
    "    #     print(f\"cur pathway {cur_pathway} has parents {cur_pathway_parents}\")\n",
    "\n",
    "    for parent in cur_pathway_parents:\n",
    "        parent_idx = pathway_ids.index(parent)\n",
    "        W2[i, parent_idx] = 1\n",
    "\n",
    "# zero diagonal (don't return self, since some pathways return themselves as level 2 parents)\n",
    "np.fill_diagonal(W2[0:(W2.shape[0] - len(go_collection)), 0:(W2.shape[0] - len(go_collection))], 0)\n",
    "\n",
    "\n",
    "W1 = np.zeros((len(parsed_pathway_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_pathway_df.index)):\n",
    "\n",
    "    if parsed_pathway_df.at[i, 'level'] <= 2:\n",
    "        cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "        cur_pathway_idx = pathway_ids.index(cur_pathway)\n",
    "\n",
    "        # get 2nd level parents\n",
    "        cur_pathway_parents = get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "\n",
    "        for parent in cur_pathway_parents:\n",
    "            parent_idx = pathway_ids.index(parent)\n",
    "            W1[i, parent_idx] = 1\n",
    "\n",
    "np.fill_diagonal(W1, 0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.119592Z",
     "start_time": "2024-05-17T18:06:47.075645Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "W[protein_ids.index('EG10230-MONOMER'), pathway_ids.index('Regulation of transcription')]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.122984Z",
     "start_time": "2024-05-17T18:06:47.120423Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# C matrix transforms complexes + monomers to just monomers.\n",
    "\n",
    "# create protein name to index mapping\n",
    "protein_name_to_index = {}\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein_name_to_index[parsed_protein_df.at[i, 'id']] = i\n",
    "\n",
    "# C matrix: complexes x proteins\n",
    "C = np.zeros((len(parsed_complex_df.index), len(parsed_protein_df.index)))\n",
    "\n",
    "for i in range(len(parsed_complex_df.index)):\n",
    "\n",
    "    complex_components = parsed_complex_df.loc[i, 'monomer_component_stoichiometry']\n",
    "\n",
    "    # TODO consider cofactors\n",
    "    # complex_cofactors = filter_complex_df.loc[i, 'cofactors']\n",
    "\n",
    "\n",
    "    for component_name, component_count in complex_components.items():\n",
    "        if component_count is not None:             # side effect of parquet\n",
    "            # get index of component in filter_protein_df\n",
    "            component_index = protein_name_to_index[component_name]\n",
    "\n",
    "            if parsed_complex_df.at[i, 'id'] == 'APORNAP-CPLX':\n",
    "                print(f'component_name: {component_name}, component_count: {component_count}, component_index: {component_index}')\n",
    "\n",
    "            C[i, component_index] = component_count\n",
    "\n",
    "# append an identity matrix to C\n",
    "C = np.concatenate((C, np.identity(len(parsed_protein_df.index))), axis=0)\n",
    "\n",
    "C_names = list(parsed_complex_df['id']) + list(parsed_protein_df['id'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.185175Z",
     "start_time": "2024-05-17T18:06:47.123646Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# P matrix transforms proteins to their respective cofactor counts.\n",
    "\n",
    "# create cofactor name to index mapping\n",
    "cofactor_name_to_index = {}\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor_name_to_index[parsed_cofactor_df.at[i, 'id']] = i\n",
    "\n",
    "cofactor_ids = list(parsed_cofactor_df['id'])\n",
    "\n",
    "# P matrix: proteins x cofactors\n",
    "P = np.zeros((len(parsed_protein_df.index), len(parsed_cofactor_df.index)))\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein_metals = parsed_protein_df.loc[i, 'metal_features_processed']\n",
    "    protein_other = parsed_protein_df.loc[i, 'other_features_processed']\n",
    "\n",
    "    for metal, count in protein_metals.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            cofactor_index = cofactor_name_to_index[metal]\n",
    "            P[i, cofactor_index] = count\n",
    "\n",
    "    for other, count in protein_other.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            cofactor_index = cofactor_name_to_index[other]\n",
    "            P[i, cofactor_index] = count\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.230052Z",
     "start_time": "2024-05-17T18:06:47.186067Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# E matrix transforms cofactors to their respective elemental composition\n",
    "\n",
    "# create list of unique elements\n",
    "unique_elements = set()\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor = parsed_cofactor_df.at[i, 'elemental_composition']\n",
    "    unique_elements.update(cofactor.keys())\n",
    "\n",
    "unique_elements = list(unique_elements)\n",
    "\n",
    "# create E matrix: cofactors x elements\n",
    "E = np.zeros((len(parsed_cofactor_df.index), len(unique_elements)))\n",
    "\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor = parsed_cofactor_df.at[i, 'elemental_composition']\n",
    "\n",
    "    for element, count in cofactor.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            element_index = unique_elements.index(element)\n",
    "            E[i, element_index] = count\n",
    "\n",
    "\n",
    "element_ids = unique_elements"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.233785Z",
     "start_time": "2024-05-17T18:06:47.230772Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# A matrix transforms proteins to their amino acid composition\n",
    "\n",
    "A = np.zeros((len(parsed_protein_df.index), len(unique_aa)))\n",
    "\n",
    "amino_acid_single_letter = list(unique_aa)\n",
    "amino_acid_ids = [AMINO_ACID_MAP[aa] for aa in amino_acid_single_letter]\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein = parsed_protein_df.at[i, 'sequence_processed']\n",
    "\n",
    "    if type(protein) is not dict:\n",
    "        continue\n",
    "\n",
    "    for aa, count in protein.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            aa_index = amino_acid_single_letter.index(aa)\n",
    "            A[i, aa_index] = count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.276900Z",
     "start_time": "2024-05-17T18:06:47.238495Z"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now ... add the counts >:o"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "time = '2500'\n",
    "date = '2024-02-17'\n",
    "experiment = 'metabolism-redux-classic-minimal'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:47.279990Z",
     "start_time": "2024-05-17T18:06:47.277976Z"
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_all = np.load(folder + '0_output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "# output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:57.446378Z",
     "start_time": "2024-05-17T18:06:47.280830Z"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output['listeners']['unique_molecule_counts']['active_RNAP']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:06:57.454732Z",
     "start_time": "2024-05-17T18:06:57.447271Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(folder + 'agent_steps.pkl', 'rb')\n",
    "agent = dill.load(f)\n",
    "f.close()\n",
    "\n",
    "metabolism = agent['ecoli-metabolism-redux-classic']\n",
    "stoichiometry = metabolism.stoichiometry\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:00.639975Z",
     "start_time": "2024-05-17T18:06:57.455537Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "initial_state = json.load(open('data/wcecoli_t0.json'))\n",
    "\n",
    "bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "\n",
    "bulk.columns = bulk_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:00.686956Z",
     "start_time": "2024-05-17T18:07:00.640720Z"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'), ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "    if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "        unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "        bulk.loc[:, bulk_id+'[c]'] += unique_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:00.692416Z",
     "start_time": "2024-05-17T18:07:00.687666Z"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bulk.loc[:, 'selC-tRNA[c]']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:00.695963Z",
     "start_time": "2024-05-17T18:07:00.693327Z"
    }
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ecocyc_to_wcm_map = {}\n",
    "\n",
    "# combined complex and protein names\n",
    "complex_protein_names = list(parsed_protein_df['id']) + list(parsed_complex_df['id'])\n",
    "\n",
    "for name in complex_protein_names:\n",
    "\n",
    "    # find complex name in bulk_ids\n",
    "    found = False\n",
    "\n",
    "    try:\n",
    "        idx = bulk_ids.index(name+'[c]')\n",
    "        ecocyc_to_wcm_map[name] = name+'[c]'\n",
    "        found = True\n",
    "        # print(f'found {complex_name} at {idx}')\n",
    "\n",
    "    except ValueError:\n",
    "        # delete key\n",
    "        found = False\n",
    "\n",
    "\n",
    "    if found == False:\n",
    "\n",
    "        for id in bulk_ids:\n",
    "            if name+'[' in id and id.startswith(name) and bulk.loc[:, id].sum() > 0:\n",
    "                #print(f'found {name} in {id} with nonzero count')\n",
    "                ecocyc_to_wcm_map[name] = id\n",
    "                found = True\n",
    "                break           # ensures preferring nonzero counts\n",
    "\n",
    "            elif name+'[' in id and id.startswith(name):\n",
    "                # print(f'found {name} in {id} with zero count')\n",
    "                ecocyc_to_wcm_map[name] = id\n",
    "                found = True\n",
    "\n",
    "    if found == False:\n",
    "        ecocyc_to_wcm_map[name] = '--TRANS-ACENAPHTHENE-12-DIOL[c]' # should be none\n",
    "        print(f'could not find {name}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:03.507470Z",
     "start_time": "2024-05-17T18:07:00.696875Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "complex_wcm_names = [ecocyc_to_wcm_map[name] for name in C_names]\n",
    "\n",
    "counts = bulk.loc[:, complex_wcm_names]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:03.524612Z",
     "start_time": "2024-05-17T18:07:03.508110Z"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "time = '1300'\n",
    "date = '2023-12-14'\n",
    "experiment = 'metabolism-redux-classic'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:03.527338Z",
     "start_time": "2024-05-17T18:07:03.525250Z"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_all = np.load(folder + '0_output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "\n",
    "bulk.columns = bulk_ids\n",
    "# output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:09.122294Z",
     "start_time": "2024-05-17T18:07:03.528044Z"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'), ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "    if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "        unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "        bulk.loc[:, bulk_id+'[c]'] += unique_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:09.126938Z",
     "start_time": "2024-05-17T18:07:09.123056Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rich_counts = bulk.loc[:, complex_wcm_names]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:09.138133Z",
     "start_time": "2024-05-17T18:07:09.127871Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "time = '5500'\n",
    "date = '2024-01-30'\n",
    "experiment = 'metabolism-redux-classic-anaerobic'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:09.140739Z",
     "start_time": "2024-05-17T18:07:09.138906Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_all = np.load(folder + '0_output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "\n",
    "bulk.columns = bulk_ids\n",
    "# output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:32.869632Z",
     "start_time": "2024-05-17T18:07:09.141523Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'), ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "    if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "        unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "        bulk.loc[:, bulk_id+'[c]'] += unique_count\n",
    "        \n",
    "anaerobic_counts = bulk.loc[:, complex_wcm_names]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:32.926517Z",
     "start_time": "2024-05-17T18:07:32.871741Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# External data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving outputs to files compatible with Julia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sum(W2[:, pathway_ids.index('Regulation of transcription')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:32.942159Z",
     "start_time": "2024-05-17T18:07:32.928470Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "[protein_ids.index('EG10230-MONOMER'), pathway_ids.index('Regulation of transcription')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:32.945274Z",
     "start_time": "2024-05-17T18:07:32.942974Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "W[protein_ids.index('EG10230-MONOMER'), pathway_ids.index('Regulation of transcription')]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:32.948036Z",
     "start_time": "2024-05-17T18:07:32.945913Z"
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "complex_names = list(parsed_complex_df['common_name'])\n",
    "protein_names = list(parsed_protein_df['common_name'])\n",
    "cofactor_names = list(parsed_cofactor_df['common_name'])\n",
    "pathway_names = list(parsed_pathway_df['common_name'])\n",
    "\n",
    "# if cplx name is nan then use id\n",
    "complex_names = [complex_names[i] if type(complex_names[i]) == str else parsed_complex_df['id'][i] for i in range(len(complex_names))]\n",
    "protein_names = [protein_names[i] if type(protein_names[i]) == str else parsed_protein_df['id'][i] for i in range(len(protein_names))]\n",
    "cofactor_names = [cofactor_names[i] if type(cofactor_names[i]) == str else parsed_cofactor_df['id'][i] for i in range(len(cofactor_names))]\n",
    "pathway_names = [pathway_names[i] if type(pathway_names[i]) == str else parsed_pathway_df['id'][i] for i in range(len(pathway_names))]\n",
    "\n",
    "# save C, P and E to julia-compatible formats\n",
    "np.savetxt('notebooks/cofactors/data/C_matrix.csv', C.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/P_matrix.csv', P.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/E_matrix.csv', E.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/W_matrix.csv', W.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/W1_matrix.csv', W1.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/W2_matrix.csv', W2.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/A_matrix.csv', A.astype(np.float64), delimiter=',')\n",
    "\n",
    "\n",
    "# write all ids to single file with each list on a new line\n",
    "with open('notebooks/cofactors/data/complex_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(complex_ids))\n",
    "with open('notebooks/cofactors/data/protein_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(protein_ids))\n",
    "with open('notebooks/cofactors/data/cofactor_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(cofactor_ids))\n",
    "with open('notebooks/cofactors/data/element_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(element_ids))\n",
    "with open('notebooks/cofactors/data/pathway_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(pathway_ids))\n",
    "with open('notebooks/cofactors/data/amino_acid_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(amino_acid_ids))\n",
    "\n",
    "# write all names to single file with each list on a new line\n",
    "with open('notebooks/cofactors/data/complex_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(complex_names))\n",
    "with open('notebooks/cofactors/data/protein_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(protein_names))\n",
    "with open('notebooks/cofactors/data/cofactor_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(cofactor_names))\n",
    "with open('notebooks/cofactors/data/pathway_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(pathway_names))\n",
    "\n",
    "\n",
    "# save counts of proteins and complexes\n",
    "np.savetxt('notebooks/cofactors/data/counts.csv', np.array(counts, dtype=np.int64), delimiter=',', fmt='%i')\n",
    "\n",
    "# save counts of proteins and complexes\n",
    "np.savetxt('notebooks/cofactors/data/rich_counts.csv', np.array(rich_counts, dtype=np.int64), delimiter=',', fmt='%i')\n",
    "\n",
    "# save counts of proteins and complexes\n",
    "np.savetxt('notebooks/cofactors/data/anaerobic_counts.csv', np.array(anaerobic_counts, dtype=np.int64), delimiter=',', fmt='%i')\n",
    "\n",
    "\n",
    "\n",
    "# save masses\n",
    "np.savetxt('notebooks/cofactors/data/monomer_masses.csv', np.array(parsed_protein_df['sequence_mass']), delimiter=',', fmt='%f')\n",
    "\n",
    "# list comprehension, if complex name in membrane_areas['id'], add membrane_ids['area_trans'], otherwise add 0 \n",
    "membrane_ids = dict(zip(membrane_proteins['id'], membrane_proteins['area_trans']))\n",
    "membrane_complex_areas = [membrane_ids[complex] if complex in membrane_ids else 0 for complex in complex_ids]\n",
    "membrane_monomer_areas = [membrane_ids[monomer] if monomer in membrane_ids else 0 for monomer in protein_ids]\n",
    "\n",
    "np.savetxt('notebooks/cofactors/data/complex_areas.csv', np.array(membrane_complex_areas), delimiter=',', fmt='%f')\n",
    "np.savetxt('notebooks/cofactors/data/monomer_areas.csv', np.array(membrane_monomer_areas), delimiter=',', fmt='%f')\n",
    "\n",
    "# save tree_matrix\n",
    "np.savetxt('notebooks/cofactors/data/tree_matrix.csv', tree_matrix, delimiter=',', fmt='%i')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:07:43.361538Z",
     "start_time": "2024-05-17T18:07:32.948700Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, entry in enumerate([f\"rich-{i}\" for i in range(1, 19)]):\n",
    "    folder = f'out/cofactors/{entry}/'\n",
    "    \n",
    "    \n",
    "    output_all = np.load(folder + '0_output.npy', allow_pickle='TRUE').item()\n",
    "    # output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "    output = output_all['agents']['0']\n",
    "    \n",
    "    fba = output['listeners']['fba_results']\n",
    "    mass = output['listeners']['mass']\n",
    "    bulk = pd.DataFrame(output['bulk'])\n",
    "    \n",
    "    fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "    exchanges = fba['estimated_exchange_dmdt']\n",
    "    \n",
    "    bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "    \n",
    "    bulk.columns = bulk_ids\n",
    "    # output['listeners']['unique_molecule_counts']['active_ribosome']\n",
    "    # in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "    for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'),\n",
    "                                ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "        if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "            unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "            bulk.loc[:, bulk_id + '[c]'] += unique_count\n",
    "            \n",
    "    if i == 0:\n",
    "        rich_counts = bulk.loc[:, complex_wcm_names].reset_index(names='timestep')\n",
    "        rich_counts[\"cell\"] = i\n",
    "    else:\n",
    "        temp = bulk.loc[:, complex_wcm_names].reset_index(names='timestep')\n",
    "        temp[\"cell\"] = i\n",
    "        rich_counts = pd.concat([rich_counts, temp])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:09:30.391022Z",
     "start_time": "2024-05-17T18:07:43.362351Z"
    }
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.savetxt('notebooks/cofactors/data/rich_counts_big.csv', np.array(rich_counts, dtype=np.int64), delimiter=',', fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:09:44.259708Z",
     "start_time": "2024-05-17T18:09:30.391909Z"
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rich_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:09:44.322685Z",
     "start_time": "2024-05-17T18:09:44.292554Z"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save counts as csv with header for Khoa\n",
    "counts.to_csv('notebooks/cofactors/data/counts_header.csv', index=False)\n",
    "\n",
    "parsed_protein_df.to_csv('notebooks/cofactors/data/protein_table.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T18:09:46.812282Z",
     "start_time": "2024-05-17T18:09:44.323618Z"
    }
   },
   "execution_count": 52,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
