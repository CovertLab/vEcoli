

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HPC Clusters &mdash; Vivarium E. coli 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8d563738"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Google Cloud" href="gcloud.html" />
    <link rel="prev" title="Documentation" href="docs.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Vivarium E. coli
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="stores.html">Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="processes.html">Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="composites.html">Composites</a></li>
<li class="toctree-l1"><a class="reference internal" href="experiments.html">Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflows.html">Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs.html">Documentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">HPC Clusters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sherlock">Sherlock</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setup">Setup</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#request-a-sherlock-account">Request a Sherlock Account</a></li>
<li class="toctree-l4"><a class="reference internal" href="#additional-resources-sherlock-documentation-from-stanford">Additional Resources: Sherlock Documentation from Stanford</a></li>
<li class="toctree-l4"><a class="reference internal" href="#login-to-sherlock">Login to Sherlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="#clone-the-vecoli-repository">Clone the vEcoli Repository</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#configuration">Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-workflows">Running Workflows</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interactive-container">Interactive Container</a></li>
<li class="toctree-l3"><a class="reference internal" href="#non-interactive-container">Non-Interactive Container</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-results-to-local-from-sherlock">Download Results to Local from Sherlock</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#other-clusters">Other Clusters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cluster-options">Cluster Options</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hyperqueue">HyperQueue</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#internal-logic">Internal Logic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring">Monitoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="#updating">Updating</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gcloud.html">Google Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci.html">Continuous Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="pycharm.html">PyCharm Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference/api_ref.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Vivarium E. coli</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">HPC Clusters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/hpc.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="hpc-clusters">
<h1>HPC Clusters<a class="headerlink" href="#hpc-clusters" title="Link to this heading"></a></h1>
<p>vEcoli uses Nextflow and Apptainer containers to run on high-performance
computing (HPC) clusters. For users with access to the Covert Lab’s partition
on Sherlock, follow the instructions in the <a class="reference internal" href="#sherlock"><span class="std std-ref">Sherlock</span></a> section. For users
looking to run the model on other HPC clusters, follow the instructions in the
<a class="reference internal" href="#other-cluster"><span class="std std-ref">Other Clusters</span></a> section.</p>
<p>To speed up HPC workflows, vEcoli supports the HyperQueue executor. See <a class="reference internal" href="#hq-info"><span class="std std-ref">HyperQueue</span></a>
for more information.</p>
<section id="sherlock">
<span id="id1"></span><h2>Sherlock<a class="headerlink" href="#sherlock" title="Link to this heading"></a></h2>
<p>On Sherlock, once a workflow is started with <a class="reference internal" href="reference/api/runscripts/runscripts.workflow.html#module-runscripts.workflow" title="runscripts.workflow"><code class="xref py py-mod docutils literal notranslate"><span class="pre">runscripts.workflow</span></code></a>,
<code class="docutils literal notranslate"><span class="pre">runscripts/container/build-image.sh</span></code> builds an Apptainer image with
a minimal snapshot of your cloned repository. Nextflow starts containers
using this image to run the steps of the workflow. To run or interact
with the model outside of <a class="reference internal" href="reference/api/runscripts/runscripts.workflow.html#module-runscripts.workflow" title="runscripts.workflow"><code class="xref py py-mod docutils literal notranslate"><span class="pre">runscripts.workflow</span></code></a>, start an
interactive container by following the steps in <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a>.</p>
<section id="setup">
<span id="sherlock-setup"></span><h3>Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following setup applies to members of the Covert Lab only.</p>
</div>
<section id="request-a-sherlock-account">
<h4>Request a Sherlock Account<a class="headerlink" href="#request-a-sherlock-account" title="Link to this heading"></a></h4>
<p>If you’ve never had a Sherlock account: Go to <a class="reference external" href="https://www.sherlock.stanford.edu/">https://www.sherlock.stanford.edu/</a> and click on <code class="docutils literal notranslate"><span class="pre">Request</span> <span class="pre">an</span> <span class="pre">Account</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Markus will have to approve this.</p>
</div>
<p>If you’ve had a Sherlock account for a previous group: Email <a class="reference external" href="mailto:srcc-support&#37;&#52;&#48;stanford&#46;edu">srcc-support<span>&#64;</span>stanford<span>&#46;</span>edu</a> and ask them to move your account to mcovert, and CC Markus on the email and in the email body ask for Markus to give approval</p>
</section>
<section id="additional-resources-sherlock-documentation-from-stanford">
<h4>Additional Resources: Sherlock Documentation from Stanford<a class="headerlink" href="#additional-resources-sherlock-documentation-from-stanford" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://srcc.stanford.edu/workshops/sherlock-boarding-session">https://srcc.stanford.edu/workshops/sherlock-boarding-session</a></p></li>
<li><p><a class="reference external" href="https://www.sherlock.stanford.edu/docs/">https://www.sherlock.stanford.edu/docs/</a></p></li>
</ul>
</section>
<section id="login-to-sherlock">
<h4>Login to Sherlock<a class="headerlink" href="#login-to-sherlock" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>&lt;YOUR_SU_NET_ID&gt;@login.sherlock.stanford.edu
<span class="c1"># Type in Stanford Password</span>
<span class="c1"># Do the Duo authentication</span>
<span class="c1"># The following setup steps should be done using the Sherlock terminal</span>
<span class="c1"># NOTE that this is a LOGIN node, so no major computing should be done here</span>

<span class="c1"># It is best to use a compute node for things like cloning the repo, running code, resetting lpad, etc</span>

srun<span class="w"> </span>-p<span class="w"> </span>mcovert<span class="w"> </span>--time<span class="o">=</span><span class="m">4</span>:00:00<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">2</span><span class="w"> </span>--pty<span class="w"> </span>bash

<span class="c1"># srun is the command for launching a job step under Slurm</span>
<span class="c1"># -p or --partition specifies which partition (queue) to use, choose covert :D</span>
<span class="c1"># --time: sets the job&#39;s wall‑clock time limit</span>
<span class="c1"># --cpus-per-task specifies # CPU cores for each task in this job step</span>
<span class="c1"># --pty: allocates a pseudo‑terminal (TTY) to run an interactive session</span>
<span class="c1"># bash: launching a Bash shell</span>
<span class="c1"># When it finished, usually you can see your JOB ID in your shell</span>

<span class="c1"># You can use scancel to abort your job step</span>
scancel<span class="w"> </span>&lt;YOUR_JOB_ID&gt;
</pre></div>
</div>
<p>You can also refer to the Sherlock Documentation: <a class="reference external" href="https://www.sherlock.stanford.edu/docs/getting-started/connecting/">https://www.sherlock.stanford.edu/docs/getting-started/connecting/</a></p>
</section>
<section id="clone-the-vecoli-repository">
<h4>Clone the vEcoli Repository<a class="headerlink" href="#clone-the-vecoli-repository" title="Link to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Git clone the vEcoli repo to your Sherlock account:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/CovertLab/vEcoli.git
</pre></div>
</div>
<p>If you have already created your branch, you can use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># View all the branches (including remote branch)</span>
git<span class="w"> </span>branch<span class="w"> </span>-a

<span class="c1"># Checkout to your own branch</span>
git<span class="w"> </span>checkout<span class="w"> </span>&lt;your_branch_name&gt;

<span class="c1"># Validate your current branch</span>
git<span class="w"> </span>branch
</pre></div>
</div>
<p>After cloning the model repository to your home directory, add the following
lines to your <code class="docutils literal notranslate"><span class="pre">~/.bash_profile</span></code>, then close and reopen your SSH connection:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load newer Git, Java (for nextflow), and Python</span>
module<span class="w"> </span>load<span class="w"> </span>system<span class="w"> </span>git<span class="w"> </span>java/21.0.4<span class="w"> </span>python/3.12.1
<span class="c1"># Include shared Nextflow and HyperQueue installations on PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$GROUP_HOME</span>/vEcoli_env
</pre></div>
</div>
<p>Then, run the following to test your setup:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>runscripts/workflow.py<span class="w"> </span>--config<span class="w"> </span>configs/test_sherlock.json
</pre></div>
</div>
<p>This will run a small workflow that:</p>
<ol class="arabic simple">
<li><p>Builds an Apptainer image with a snapshot of your cloned repository.</p></li>
<li><p>Runs the ParCa.</p></li>
<li><p>Runs one simulation.</p></li>
<li><p>Runs the mass fraction analysis.</p></li>
</ol>
<p>All output files will be saved to a <code class="docutils literal notranslate"><span class="pre">test_sherlock</span></code> directory in your
cloned repository. You can modify the workflow output directory by changing
the <code class="docutils literal notranslate"><span class="pre">out_dir</span></code> option under <code class="docutils literal notranslate"><span class="pre">emitter_arg</span></code> in the config JSON.
See <a class="reference internal" href="#sherlock-config"><span class="std std-ref">Configuration</span></a> for a description of the Sherlock-specific
configuration options and <a class="reference internal" href="#sherlock-running"><span class="std std-ref">Running Workflows</span></a> for details about running
a workflow on Sherlock.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">test_sherlock.json</span></code> sets <code class="docutils literal notranslate"><span class="pre">out_dir</span></code> to <code class="docutils literal notranslate"><span class="pre">.</span></code>. In relative path syntax,
this refers to the current directory, meaning the cloned repo. This makes
the configuration portable as it does not assume the presence of any other
folders. However, as noted in <a class="reference internal" href="#sherlock-config"><span class="std std-ref">Configuration</span></a>, we recommend changing
this in your workflows.</p>
</div>
<p>To run scripts on Sherlock outside a workflow, see <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a>.
To run scripts on Sherlock through a SLURM batch script, see <a class="reference internal" href="#sherlock-noninteractive"><span class="std std-ref">Non-Interactive Container</span></a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">nano</span></code> as text editor:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nano<span class="w"> </span>~/.bash_profile
<span class="c1"># After writing, you can use Ctrl+O to write out, Enter to confirm, and Ctrl+X to exit</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If you choose to use <code class="docutils literal notranslate"><span class="pre">vim</span></code>, press <code class="docutils literal notranslate"><span class="pre">i</span></code> for insert, and press <code class="docutils literal notranslate"><span class="pre">Esc</span></code>, then type <code class="docutils literal notranslate"><span class="pre">:wq</span></code> and Enter for writing out</p></li>
<li><p>Before running the <code class="docutils literal notranslate"><span class="pre">python3</span></code> to set up the env, ensure you are in the vEcoli repo</p></li>
<li><p>It usually takes time to run first job</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above setup is sufficient to run workflows on Sherlock. However, if you
have a compelling reason to update the shared Nextflow or HyperQueue binaries,
navigate to <code class="docutils literal notranslate"><span class="pre">$GROUP_HOME/vEcoli_env</span></code> and run:</p>
<ol class="arabic simple">
<li><p>Nextflow: <code class="docutils literal notranslate"><span class="pre">NXF_EDGE=1</span> <span class="pre">nextflow</span> <span class="pre">self-update</span></code></p></li>
<li><p>HyperQueue: See <a class="reference internal" href="#hq-info"><span class="std std-ref">HyperQueue</span></a>.</p></li>
</ol>
<p>Then, reset the permissions of the updated binaries with <code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">777</span> <span class="pre">*</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Before building your own config file and running an experiment, remember:</p>
<p>Python scripts (other than runscripts/workflow.py) <strong>WILL NOT</strong> run on Sherlock directly.
This includes the standalone ParCa, simulation, and analysis run scripts.
Instead, these scripts can be run inside an <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a> (ideal for script development or debugging)
or <a class="reference internal" href="#sherlock-noninteractive"><span class="std std-ref">Non-Interactive Container</span></a> (ideal for longer or more resource-intensive scripts that do not require user input).</p>
</div>
</section>
</section>
<section id="configuration">
<span id="sherlock-config"></span><h3>Configuration<a class="headerlink" href="#configuration" title="Link to this heading"></a></h3>
<p>To tell vEcoli that you are running on Sherlock, you MUST include the following
keys in your configuration JSON (note the top-level <code class="docutils literal notranslate"><span class="pre">sherlock</span></code> key):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;sherlock&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="c1"># Boolean, whether to build a fresh Apptainer image. If files that are</span>
    <span class="c1"># not excluded by .dockerignore did not change since your last build,</span>
    <span class="c1"># you can set this to false to skip building the image. DO NOT set this</span>
    <span class="c1"># to a location in the cloned repo or else the resulting image(s) will be</span>
    <span class="c1"># included in future image builds. test_sherlock.json is an exception</span>
    <span class="c1"># because the test_sherlock folder is ignored by .dockerignore.</span>
    <span class="s2">&quot;build_image&quot;</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
    <span class="c1"># Path (relative or absolute, including file name) of Apptainer image to</span>
    <span class="c1"># build (or use directly, if build_image is false)</span>
    <span class="s2">&quot;container_image&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="c1"># Boolean, whether to run using HyperQueue.</span>
    <span class="s2">&quot;hyperqueue&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
    <span class="c1"># Boolean, denotes that a workflow is being run as part of Jenkins</span>
    <span class="c1"># continuous integration testing. Randomizes the initial seed and</span>
    <span class="c1"># ensures that all STDOUT and STDERR is piped to the launching process</span>
    <span class="c1"># so they can be reported by GitHub</span>
    <span class="s2">&quot;jenkins&quot;</span><span class="p">:</span> <span class="n">false</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In addition to these options, you <strong>MUST</strong> set the emitter output directory
(see description of <code class="docutils literal notranslate"><span class="pre">emitter_arg</span></code> in <a class="reference internal" href="experiments.html#json-config"><span class="std std-ref">JSON Config Files</span></a>) to a path with
enough space to store your workflow outputs.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We recommend setting <code class="docutils literal notranslate"><span class="pre">out_dir</span></code> under <code class="docutils literal notranslate"><span class="pre">emitter_arg</span></code> to a location in your
<code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory to circumvent the <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> storage limit
(run <code class="docutils literal notranslate"><span class="pre">sh_quota</span></code> to view). One way to do this is using an absolute path
(e.g. <code class="docutils literal notranslate"><span class="pre">/scratch/users/{username}</span></code>). Alternatively, you can create a
symlink to your scratch directory by running the following command inside
your cloned repository: <code class="docutils literal notranslate"><span class="pre">ln</span> <span class="pre">-s</span> <span class="pre">/scratch/users/{username}</span> <span class="pre">out</span></code> (delete
<code class="docutils literal notranslate"><span class="pre">out</span></code> in your cloned repo first if it already exists). Then, using <code class="docutils literal notranslate"><span class="pre">out</span></code>
for <code class="docutils literal notranslate"><span class="pre">out_dir</span></code> will cause all simulation output to be redirected to your
scratch directory.</p>
</div>
<p>If using the Parquet emitter and <code class="docutils literal notranslate"><span class="pre">threaded</span></code> is not set to false under
<code class="docutils literal notranslate"><span class="pre">emitter_arg</span></code>, a warning will be printed suggesting that you set <code class="docutils literal notranslate"><span class="pre">threaded</span></code>
to false. This ensures that simulations use only a single CPU core, the default
that is allocated per simulation on Sherlock (regardless of whether HyperQueue
is used). On Sherlock, storage speed is not a bottleneck, so performance with
<code class="docutils literal notranslate"><span class="pre">threaded</span></code> set to false and 1 core per simulation is comparable to running
with <code class="docutils literal notranslate"><span class="pre">threaded</span></code> unset (default: true) and 2 cores per simulation.</p>
<p>If storage speed was a bottleneck, the additional thread would allow
simulation execution to continue while Polars writes Parquet files to disk.
To properly take advantage of this, you would also need to increase the number
of cores per simulation to 2 by modifying the <code class="docutils literal notranslate"><span class="pre">cpus</span></code> directive under the
<code class="docutils literal notranslate"><span class="pre">sherlock</span></code> and <code class="docutils literal notranslate"><span class="pre">sherlock_hq</span></code> profiles in <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/config.template</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">~</span></code> and environment variables like <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> are not expanded in the
configuration JSON. See the warning box at <a class="reference internal" href="workflows.html"><span class="doc">Workflows</span></a>.</p>
</div>
</section>
<section id="running-workflows">
<span id="sherlock-running"></span><h3>Running Workflows<a class="headerlink" href="#running-workflows" title="Link to this heading"></a></h3>
<p>With these options in the configuration JSON, a workflow can be started by
running <code class="docutils literal notranslate"><span class="pre">python3</span> <span class="pre">runscripts/workflow.py</span> <span class="pre">--config</span> <span class="pre">{}</span></code>, substituting
in the path to your config JSON.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Remember to use <code class="docutils literal notranslate"><span class="pre">python3</span></code> to start workflows instead of <code class="docutils literal notranslate"><span class="pre">python</span></code>.
This command is supposed to run on <strong>login node</strong>, which means there is no need to use <code class="docutils literal notranslate"><span class="pre">srun</span></code> to request a <strong>compute node</strong>.
If there is trouble with permission denied for nextflow (you can use <code class="docutils literal notranslate"><span class="pre">nextflow</span> <span class="pre">-version</span></code> to check out), you can try <code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">a+rwx</span></code></p>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">build_image</span></code> is true in your config JSON, the terminal will report that
a SLURM job was submitted to build the container image. When the image build
job starts, the terminal will report the build progress.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Files that match the patterns in <code class="docutils literal notranslate"><span class="pre">.dockerignore</span></code> are excluded from the image.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the Apptainer build fails, eg:
<code class="docutils literal notranslate"><span class="pre">FATAL:</span>&#160;&#160; <span class="pre">While</span> <span class="pre">performing</span> <span class="pre">build:</span> <span class="pre">conveyor</span> <span class="pre">failed</span> <span class="pre">to</span> <span class="pre">get:</span> <span class="pre">unexpected</span> <span class="pre">end</span> <span class="pre">of</span> <span class="pre">JSON</span> <span class="pre">input</span></code>,
try cleaning cache: <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">cache</span> <span class="pre">clean</span></code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not make any changes to your cloned repository or close your SSH
connection until the build has finished.</p>
</div>
<p>Once the build has finished, the terminal will report that a <strong>SLURM job</strong>
was submitted for the Nextflow workflow orchestrator before exiting
back to the shell. At this point, you are free to close your connection,
start additional workflows, etc. You can use <code class="docutils literal notranslate"><span class="pre">squeue</span></code> to view the status of your SLURM job:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># View by job</span>
squeue<span class="w"> </span>-j<span class="w"> </span>&lt;Your_Job_ID&gt;
<span class="c1"># View by user</span>
squeue<span class="w"> </span>-u<span class="w"> </span>&lt;Your_user_name&gt;
</pre></div>
</div>
<p>Unlike workflows run locally, Sherlock’s
containerized workflows mean any changes made to the repository after the
container image has been built will not affect the running workflow.</p>
<p>Once started, the Nextflow job will stay alive for the duration of the
workflow (up to 7 days) and submit new SLURM jobs as needed.</p>
<p>If you are trying to run a workflow that takes longer than 7 days, you can
use the resume functionality (see <a class="reference internal" href="workflows.html#fault-tolerance"><span class="std std-ref">Fault Tolerance</span></a>). Alternatively,
consider running your workflow on Google Cloud, which has no maximum workflow
runtime (see <a class="reference internal" href="gcloud.html"><span class="doc">Google Cloud</span></a>).</p>
<p>You can start additional, concurrent workflows that each build a new image
with different modifications to the cloned repository. However, if possible,
we recommend designing your code to accept options through the config JSON
which modify the behavior of your workflow without modifying core code. This
allows you to save time by reusing a previously built image as follows:
set <code class="docutils literal notranslate"><span class="pre">build_image</span></code> to false and <code class="docutils literal notranslate"><span class="pre">container_image</span></code> to the path of said image.</p>
<p>There is a 4 hour time limit on each job in the workflow, including analyses.
This is a generous limit designed to accomodate very slow-dividing cells.
Generally, we recommend that users exclude analysis scripts which take more
than a few minutes from their workflow configuration. Instead, either run these
manually following <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a> or create a
SLURM batch script to run these analyses following <a class="reference internal" href="#sherlock-noninteractive"><span class="std std-ref">Non-Interactive Container</span></a>.</p>
</section>
<section id="interactive-container">
<span id="sherlock-interactive"></span><h3>Interactive Container<a class="headerlink" href="#interactive-container" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The following steps should be run on a compute node. See the
<a class="reference external" href="https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/?h=interactive#interactive-jobs">Sherlock documentation</a>
for details.</p>
</div>
<p>The maximum resource request for an interactive compute
node is 2 hours, 4 CPU cores, and 8GB RAM/core. Scripts that require more
resources should be submitted as SLURM batch scripts to the <code class="docutils literal notranslate"><span class="pre">mcovert</span></code>
or <code class="docutils literal notranslate"><span class="pre">owners</span></code> partition (see <a class="reference internal" href="#sherlock-noninteractive"><span class="std std-ref">Non-Interactive Container</span></a>).</p>
<p>To run scripts on Sherlock, you must have either:</p>
<ul class="simple">
<li><p>Previously run a workflow on Sherlock and have access to the built container image</p></li>
<li><p>Built a container image manually using <code class="docutils literal notranslate"><span class="pre">runscripts/container/build-image.sh</span></code> with
the <code class="docutils literal notranslate"><span class="pre">-a</span></code> flag</p></li>
</ul>
<p>Start an interactive container with your full image path (see the warning box at
<a class="reference internal" href="workflows.html"><span class="doc">Workflows</span></a>) by navigating to your cloned repository and running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>runscripts/container/interactive.sh<span class="w"> </span>-i<span class="w"> </span>container_image<span class="w"> </span>-a
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Inside the interactive container, you can safely use <code class="docutils literal notranslate"><span class="pre">python</span></code> directly
in addition to the usual <code class="docutils literal notranslate"><span class="pre">uv</span></code> commands.</p>
</div>
<p>The above command launches a container containing a snapshot of your
cloned repository as it was when the image was built. This snapshot
is located at <code class="docutils literal notranslate"><span class="pre">/vEcoli</span></code> inside the container and is mostly intended
to guarantee reproducibility for troubleshooting failed workflow jobs.
More specifically, users who wish to debug a failed workflow job should:</p>
<ol class="arabic simple">
<li><p>Start an interactive container with the image used to run the workflow.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">nano</span></code> to add breakpoints (<code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">ipdb;</span> <span class="pre">ipdb.set_trace()</span></code>)
to the relevant scripts in <code class="docutils literal notranslate"><span class="pre">/vEcoli</span></code>.</p></li>
<li><p>Navigate to the working directory (see <a class="reference internal" href="workflows.html#troubleshooting"><span class="std std-ref">Troubleshooting</span></a>) for the
job that you want to debug.</p></li>
<li><p>Invoke <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">.command.sh</span></code> to run the failing task and pause upon
reaching your breakpoints, allowing you to inspect variables and step
through the code.</p></li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">~</span></code> and environment variables like <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> do not work
inside the container. Follow the instructions in the warning box at
<a class="reference internal" href="workflows.html"><span class="doc">Workflows</span></a> <strong>outside</strong> the container to get the full path to
use inside the container.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>Any changes that you make to <code class="docutils literal notranslate"><span class="pre">/vEcoli</span></code> inside the container are discarded
when the container terminates.</p>
</div>
<p>Moreover, if you want to exit the interactive image, just type <code class="docutils literal notranslate"><span class="pre">exit</span></code> command.</p>
<p>To start an interactive container that reflects the current state of your
cloned repository, navigate to your cloned repository and run the above
command with the <code class="docutils literal notranslate"><span class="pre">-d</span></code> flag to start a “development” container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>runscripts/container/interactive.sh<span class="w"> </span>-i<span class="w"> </span>container_image<span class="w"> </span>-a<span class="w"> </span>-d
</pre></div>
</div>
<p>In this mode, instead of editing source files in <code class="docutils literal notranslate"><span class="pre">/vEcoli</span></code>, you can
directly edit the source files in your cloned repository and have those
changes immediately reflected when running those scripts inside the
container. Because you are just modifying your cloned repository, any
code changes you make will persist after the container terminates and
can be tracked using Git version control.</p>
</section>
<section id="non-interactive-container">
<span id="sherlock-noninteractive"></span><h3>Non-Interactive Container<a class="headerlink" href="#non-interactive-container" title="Link to this heading"></a></h3>
<p>To run any script inside a container without starting an interactive session,
use the same command as <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a> but specify a command
using the <code class="docutils literal notranslate"><span class="pre">-c</span></code> flag. For example, to run the ParCa process, navigate to
your cloned repository and run the following command, replacing <code class="docutils literal notranslate"><span class="pre">container_image</span></code>
with the path to your container image and <code class="docutils literal notranslate"><span class="pre">{}</span></code> with the path to your
configuration JSON:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>runscripts/container/interactive.sh<span class="w"> </span>-i<span class="w"> </span>container_image<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;python /vEcoli/runscripts/parca.py --config {}&quot;</span>
</pre></div>
</div>
<p>This feature is intended for use in
<a class="reference external" href="https://www.sherlock.stanford.edu/docs/getting-started/submitting/#batch-scripts">SLURM batch scripts</a>
to manually run analysis scripts with custom resource requests. Make sure
to include one of the following directives at the top of your script:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=owners</span></code>: This is the largest partition on Sherlock and
the most likely to have free resources available for job scheduling. Even so,
queue times are variable, and other users may preempt your job at any moment,
though this is anecdotally rare for small jobs under an hour long.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=mcovert</span></code>: Best for high priority scripts (short queue time)
that you cannot risk being preempted. The number of available cores is 32 minus
whatever is currently being used by other users in the <code class="docutils literal notranslate"><span class="pre">mcovert</span></code> partition.
Importantly, if all 32 cores are in use by <code class="docutils literal notranslate"><span class="pre">mcovert</span></code> users, not only will your
script have to wait for resources to free up, so will any workflows. As such,
treat this partition as a limited resource reserved for high priority jobs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=normal</span></code>: Potentially longer queue time than either of the
two options above but no risk of preemption.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=owners,normal</span></code>: Uses either the <code class="docutils literal notranslate"><span class="pre">owners</span></code> or <code class="docutils literal notranslate"><span class="pre">normal</span></code>
partition. This is the recommended option for the vast majority of scripts.</p></li>
</ul>
<p>Following is a sample of sbatch scripts for requiring more resources to analysis simulation results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=analysis_job</span>
<span class="c1">#SBATCH --output=analysis_job.%j.out</span>
<span class="c1">#SBATCH --error=analysis_job.%j.err</span>
<span class="c1">#SBATCH --time=20:00</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --partition=owners,normal</span>
<span class="c1">#SBATCH --cpus-per-task=4</span>
<span class="c1">#SBATCH --mem=64GB</span>

srun<span class="w"> </span>runscripts/container/interactive.sh<span class="w"> </span>-i<span class="w"> </span>&lt;Your_image_path&gt;<span class="w">  </span>-a<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;python runscripts/analysis.py --config &lt;Your_config_file_path&gt;&quot;</span>
</pre></div>
</div>
<p>Then, use <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> to submit the job:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>&lt;Your_Job_Name&gt;.sh
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.err</span></code> and <code class="docutils literal notranslate"><span class="pre">.out</span></code> files will be created in the same directory as the sbatch script.</p>
<p>Just as with interactive containers, to run scripts directly from your
cloned repository and not the snapshot, add the <code class="docutils literal notranslate"><span class="pre">-d</span></code> flag and drop the
<code class="docutils literal notranslate"><span class="pre">/vEcoli/</span></code> prefix from script names. Note that changing files in your
cloned repository may affect SLURM batch jobs submitted with this flag.</p>
</section>
<section id="download-results-to-local-from-sherlock">
<span id="id2"></span><h3>Download Results to Local from Sherlock<a class="headerlink" href="#download-results-to-local-from-sherlock" title="Link to this heading"></a></h3>
<p>It’s recommended to turn to
<a class="reference external" href="https://www.sherlock.stanford.edu/docs/storage/data-transfer/">Sherlock’s Data Transfer documentation</a>
for details on transferring files to and from your local machine.</p>
<p>Following are common methods <code class="docutils literal notranslate"><span class="pre">scp</span></code> and <code class="docutils literal notranslate"><span class="pre">rsync</span></code>:</p>
<p><code class="docutils literal notranslate"><span class="pre">scp</span></code> is convenient for downloading files from the cluster. You can simply execute the following on your <strong>local terminal</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># -r for recursively duplicate the whole repo:</span>
scp<span class="w"> </span>-r<span class="w"> </span>&lt;Your_SU_ID&gt;@login.sherlock.stanford.edu:/path/to/remote/folder<span class="w">  </span>/path/to/local/destination

<span class="c1"># If you only want to download single file:</span>
scp<span class="w">  </span>&lt;Your_SU_ID&gt;@login.sherlock.stanford.edu:/path/to/remote/file<span class="w">  </span>/path/to/local/destination/
</pre></div>
</div>
<p>In practice, usually we want to get the analytical results for our simulation.
Due to the report files being HTML files typically, we can turn to shell wildcard and use <code class="docutils literal notranslate"><span class="pre">rsync</span></code> with <code class="docutils literal notranslate"><span class="pre">include/exclude</span></code> filters:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Recursively downloads all .html files under the specific directory on Sherlock</span>
<span class="c1"># to your local machine while preserving the subdirectory structure:</span>

rsync<span class="w"> </span>-av<span class="w"> </span>--prune-empty-dirs<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--include<span class="o">=</span><span class="s1">&#39;*/&#39;</span><span class="w"> </span>--include<span class="o">=</span><span class="s1">&#39;*.html&#39;</span><span class="w"> </span>--exclude<span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>&lt;Your_SU_ID&gt;@login.sherlock.stanford.edu:/path/to/remote/folder<span class="w">  </span>/path/to/local/destination

<span class="c1"># --include=&#39;*/&#39;: Keeps all directories, allowing rsync to traverse into subdirectories</span>
<span class="c1"># --include=&#39;*.html&#39;: Includes only .html files</span>
<span class="c1"># --exclude=&#39;*&#39;: Excludes everything else</span>
<span class="c1"># -a: Archive mode (preserves metadata)</span>
<span class="c1"># -v: Verbose output</span>
<span class="c1"># --prune-empty-dirs: Avoids creating empty directories on the local machine</span>
</pre></div>
</div>
<p>Both <code class="docutils literal notranslate"><span class="pre">scp</span></code> and <code class="docutils literal notranslate"><span class="pre">rsync</span></code> will require your password and Duo validation.</p>
</section>
</section>
<section id="other-clusters">
<span id="other-cluster"></span><h2>Other Clusters<a class="headerlink" href="#other-clusters" title="Link to this heading"></a></h2>
<p>Nextflow has support for a wide array of HPC schedulers. If your HPC cluster uses
a supported scheduler, you can likely run vEcoli on it with fairly minimal modifications.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h3>
<p>The following are required:</p>
<ul class="simple">
<li><p>Nextflow (requires Java)</p></li>
<li><p>Python 3.9+</p></li>
<li><p>Git clone vEcoli to a location that is accessible from all nodes in your cluster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_dir</span></code> under <code class="docutils literal notranslate"><span class="pre">emitter_arg</span></code> set to a location that is accessible from all
nodes in your cluster</p></li>
</ul>
<p>If your cluster has Apptainer (formerly known as Singularity) installed,
check to see if it is configured to automatically mount the filesystem of
<code class="docutils literal notranslate"><span class="pre">out_dir</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>). If not, you will need to add <code class="docutils literal notranslate"><span class="pre">-B</span> <span class="pre">/full/path/to/out_dir</span></code>
to the <code class="docutils literal notranslate"><span class="pre">containerOptions</span></code> directives in <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/config.template</span></code>,
substituting in the absolute path to <code class="docutils literal notranslate"><span class="pre">out_dir</span></code>. Additionally, you will need to
manually specify the same paths when running interactive containers
(see <a class="reference internal" href="#sherlock-interactive"><span class="std std-ref">Interactive Container</span></a>) using the <code class="docutils literal notranslate"><span class="pre">-p</span></code> option.</p>
<p>If your cluster does not have Apptainer, you can try the following steps:</p>
<ol class="arabic simple">
<li><p>Completely follow the local setup instructions in the README (install uv, etc).</p></li>
<li><p>Delete the following lines from <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/config.template</span></code>:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>process.container<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;IMAGE_NAME&#39;</span>
...
apptainer.enabled<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Make sure to always set <code class="docutils literal notranslate"><span class="pre">build_runtime_image</span></code> to false in your config JSONs
(see <a class="reference internal" href="#sherlock-config"><span class="std std-ref">Configuration</span></a>)</p></li>
</ol>
</section>
<section id="cluster-options">
<span id="id3"></span><h3>Cluster Options<a class="headerlink" href="#cluster-options" title="Link to this heading"></a></h3>
<p>If your HPC cluster uses the SLURM scheduler,
you can use vEcoli on that cluster by changing the <code class="docutils literal notranslate"><span class="pre">queue</span></code> option in
<code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/config.template</span></code> and <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/template.nf</span></code>
and all instances of <code class="docutils literal notranslate"><span class="pre">--partition=QUEUE(S)</span></code> in <a class="reference internal" href="reference/api/runscripts/runscripts.workflow.html#module-runscripts.workflow" title="runscripts.workflow"><code class="xref py py-mod docutils literal notranslate"><span class="pre">runscripts.workflow</span></code></a>
to the right queue(s) for your cluster. Also, remove the <code class="docutils literal notranslate"><span class="pre">--prefer=&quot;CPU_GEN...</span></code>
<code class="docutils literal notranslate"><span class="pre">clusterOptions</span></code> in those same files.</p>
<p>If your HPC cluster uses a different scheduler, refer to the Nextflow
<a class="reference external" href="https://www.nextflow.io/docs/latest/executor.html">executor documentation</a>
for more information on configuring the right executor. Beyond the changes above,
you will at least need to modify the <code class="docutils literal notranslate"><span class="pre">executor</span></code> directives for the <code class="docutils literal notranslate"><span class="pre">sherlock</span></code>
and <code class="docutils literal notranslate"><span class="pre">sherlock_hq</span></code> profiles in <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/config.template</span></code> and for the
<code class="docutils literal notranslate"><span class="pre">hqWorker</span></code> process in <code class="docutils literal notranslate"><span class="pre">runscripts/nextflow/template.nf</span></code>. Additionally, you will
need to replace the SLURM submission directives in <a class="reference internal" href="reference/api/runscripts/runscripts.workflow.html#runscripts.workflow.main" title="runscripts.workflow.main"><code class="xref py py-func docutils literal notranslate"><span class="pre">runscripts.workflow.main()</span></code></a>
with equivalent directives for your scheduler.</p>
</section>
</section>
<section id="hyperqueue">
<span id="hq-info"></span><h2>HyperQueue<a class="headerlink" href="#hyperqueue" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://it4innovations.github.io/hyperqueue/stable/">HyperQueue</a> consists of a
head server and one or more workers allocated by the underlying HPC scheduler. By
configuring the worker jobs to persist for long enough to complete multiple tasks,
HyperQueue reduces the amount of time spent waiting in the queue, which is especially
important for workflows with numerous shorter tasks like ours. We recommend using
HyperQueue for all workflows that span more than a handful of generations.</p>
<section id="internal-logic">
<h3>Internal Logic<a class="headerlink" href="#internal-logic" title="Link to this heading"></a></h3>
<p>If the <code class="docutils literal notranslate"><span class="pre">hyperqueue</span></code> option is set to true under the <code class="docutils literal notranslate"><span class="pre">sherlock</span></code> key in the
configuration JSON used to run <code class="docutils literal notranslate"><span class="pre">runscripts/workflow.py</span></code>, the following steps
will occur in order:</p>
<ol class="arabic simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">build_image</span></code> is True, submit a SLURM job to build the container image.</p></li>
<li><p>Submit a single long-running SLURM job on the dedicated Covert Lab partition
to run both Nextflow and the HyperQueue head server.</p></li>
<li><p>Start the HyperQueue head server (initially no workers).</p></li>
<li><p>Nextflow submits a SLURM job to run the ParCa then another to create variants.
Both must finish for Nextflow to calculate the maximum number of concurrent
simulations <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">seeds</span> <span class="pre">*</span> <span class="pre">#</span> <span class="pre">variants</span></code>.</p></li>
<li><p>Nextflow submits SLURM jobs to start <code class="docutils literal notranslate"><span class="pre">(#</span> <span class="pre">seeds</span> <span class="pre">*</span> <span class="pre">#</span> <span class="pre">variants)</span> <span class="pre">//</span> <span class="pre">4</span></code> HyperQueue
workers, each worker with 4 cores, 16GB RAM, and a 24 hour limit. A
proportionally smaller worker is potentially created to handle the remainder
(e.g. for 2 leftover, 2 cores, 8GB RAM, and same 24 hour limit).</p></li>
<li><p>Nextflow submits simulation tasks to the HyperQueue head server, which schedules
them on the available workers.</p></li>
<li><p>Nextflow submits analysis tasks to SLURM directly as they do not hold up the
workflow and can wait for a bit in the queue. This increases simulation
throughput by dedicating all HyperQueue worker resources to running simulations.</p></li>
<li><p>If any HyperQueue worker job terminates with one of three exit codes
(see <a class="reference internal" href="workflows.html#fault-tolerance"><span class="std std-ref">Fault Tolerance</span></a>), it is resubmitted by Nextflow to maintain
the optimal number of workers for parallelizing the workflow.</p></li>
<li><p>As lineages fail and/or complete, the number of concurrent simulations decreases
and HyperQueue workers start to go idle. Idle workers automatically terminate
after 5 minutes of inactivity.</p></li>
<li><p>Upon completion of the Nextflow workflow, the HyperQueue head server terminates
any remaining workers and exits.</p></li>
</ol>
</section>
<section id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Link to this heading"></a></h3>
<p>As long as <code class="docutils literal notranslate"><span class="pre">--server-dir</span></code> is given as described below, the <code class="docutils literal notranslate"><span class="pre">hq</span></code> command can be
run on any node to monitor the status of the HyperQueue workers and jobs
for a given workflow
(<a class="reference external" href="https://it4innovations.github.io/hyperqueue/latest/cheatsheet/">cheatsheet</a>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace OUTDIR with the output directory and EXPERIMENT_ID with the</span>
<span class="c1"># experiment ID from your configuration JSON.</span>

<span class="c1"># Get HyperQueue JOB_ID from this list of jobs</span>
hq<span class="w"> </span>--server-dir<span class="w"> </span>OUTDIR/EXPERIMENT_ID/nextflow/.hq-server<span class="w"> </span>job<span class="w"> </span>list

<span class="c1"># Get more detailed information about a specific job by ID, including</span>
<span class="c1"># its work directory, runtime, and node</span>
hq<span class="w"> </span>--server-dir<span class="w"> </span>OUTDIR/EXPERIMENT_ID/nextflow/.hq-server<span class="w"> </span>job<span class="w"> </span>info<span class="w"> </span>JOB_ID
</pre></div>
</div>
</section>
<section id="updating">
<h3>Updating<a class="headerlink" href="#updating" title="Link to this heading"></a></h3>
<p>HyperQueue is distributed as a pre-built binary on GitHub.
Unfortunately, this binary is built with a newer version of GLIBC than the
one available on Sherlock, necessitating a rebuild from source. A binary built
in this way is available in <code class="docutils literal notranslate"><span class="pre">$GROUP_HOME/vEcoli_env</span></code> to users with access to
the Covert Lab’s partition on Sherlock. This is added to <code class="docutils literal notranslate"><span class="pre">PATH</span></code> in the
Sherlock setup instructions, so no further action is required.</p>
<p>Users who want or need to build from source should follow
<a class="reference external" href="https://it4innovations.github.io/hyperqueue/stable/installation/#compilation-from-source-code">these instructions</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="docs.html" class="btn btn-neutral float-left" title="Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gcloud.html" class="btn btn-neutral float-right" title="Google Cloud" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2025, The Vivarium E. coli Authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>